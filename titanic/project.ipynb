{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Titanic competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "import seaborn as sns; sns.set()\n",
    "import visuals as vs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import pairwise_distances_argmin, silhouette_score, accuracy_score, confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers dataset has 891 samples with 12 features each.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    dataset = pd.read_csv(\"data/train.csv\")\n",
    "    print(\"Passengers dataset has {} samples with {} features each.\".format(*dataset.shape))\n",
    "except Exception as e:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing? \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a first look to dataset. We are interested in knowing number, type, range of values and empty parameters in order to take further decisions about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check how many empty values are in each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['NameLength'] = dataset.apply(lambda row: len(row.Name), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin cabina 0.4282744282744283\n",
      "Con cabina 2.0\n"
     ]
    }
   ],
   "source": [
    "dataset.head(10)\n",
    "vivos = len(dataset.loc[(dataset['Survived'] == 1) & (dataset['Cabin'].isnull())])\n",
    "muertos = len(dataset.loc[(dataset['Survived'] == 0) & (dataset['Cabin'].isnull())])\n",
    "#Sin cabina\n",
    "print('Sin cabina', vivos/muertos)\n",
    "vivos = len(dataset.loc[(dataset['Survived'] == 1) & (dataset['Cabin'].notnull())])\n",
    "muertos = len(dataset.loc[(dataset['Survived'] == 0) & (dataset['Cabin'].notnull())])\n",
    "print('Con cabina', vivos/muertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunm:  PassengerId ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Survived ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Pclass ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Name ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Sex ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Age ** Empty :  177 ** Empty percentage:  0.19865319865319866\n",
      "Colunm:  SibSp ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Parch ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Ticket ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Fare ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Cabin ** Empty :  687 ** Empty percentage:  0.7710437710437711\n",
      "Colunm:  Embarked ** Empty :  2 ** Empty percentage:  0.002244668911335578\n",
      "Colunm:  NameLength ** Empty :  0 ** Empty percentage:  0.0\n"
     ]
    }
   ],
   "source": [
    "for (columnName, columnData) in dataset.iteritems():\n",
    "   empty_values = columnData.isna().sum()\n",
    "   print('Colunm: ', columnName, '** Empty : ', empty_values, '** Empty percentage: ', empty_values/len(columnData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* PassengerId **************\n",
      "891\n",
      "             Survived  Pclass   Age  SibSp  Parch     Fare  NameLength\n",
      "PassengerId                                                           \n",
      "1                   0       3  22.0      1      0   7.2500          23\n",
      "2                   1       1  38.0      1      0  71.2833          51\n",
      "3                   1       3  26.0      0      0   7.9250          22\n",
      "4                   1       1  35.0      1      0  53.1000          44\n",
      "5                   0       3  35.0      0      0   8.0500          24\n",
      "...               ...     ...   ...    ...    ...      ...         ...\n",
      "887                 0       2  27.0      0      0  13.0000          21\n",
      "888                 1       1  19.0      0      0  30.0000          28\n",
      "889                 0       3   NaN      1      2  23.4500          40\n",
      "890                 1       1  26.0      0      0  30.0000          21\n",
      "891                 0       3  32.0      0      0   7.7500          19\n",
      "\n",
      "[891 rows x 7 columns]\n",
      "************* Survived **************\n",
      "2\n",
      "          PassengerId    Pclass        Age     SibSp     Parch       Fare  \\\n",
      "Survived                                                                    \n",
      "0          447.016393  2.531876  30.626179  0.553734  0.329690  22.117887   \n",
      "1          444.368421  1.950292  28.343690  0.473684  0.464912  48.395408   \n",
      "\n",
      "          NameLength  \n",
      "Survived              \n",
      "0          24.531876  \n",
      "1          30.871345  \n",
      "************* Pclass **************\n",
      "3\n",
      "        PassengerId  Survived        Age     SibSp     Parch       Fare  \\\n",
      "Pclass                                                                    \n",
      "1        461.597222  0.629630  38.233441  0.416667  0.356481  84.154687   \n",
      "2        445.956522  0.472826  29.877630  0.402174  0.380435  20.662183   \n",
      "3        439.154786  0.242363  25.140620  0.615071  0.393075  13.675550   \n",
      "\n",
      "        NameLength  \n",
      "Pclass              \n",
      "1        29.949074  \n",
      "2        28.217391  \n",
      "3        25.183299  \n",
      "************* Name **************\n",
      "891\n",
      "                                       PassengerId  Survived  Pclass   Age  \\\n",
      "Name                                                                         \n",
      "Abbing, Mr. Anthony                            846         0       3  42.0   \n",
      "Abbott, Mr. Rossmore Edward                    747         0       3  16.0   \n",
      "Abbott, Mrs. Stanton (Rosa Hunt)               280         1       3  35.0   \n",
      "Abelson, Mr. Samuel                            309         0       2  30.0   \n",
      "Abelson, Mrs. Samuel (Hannah Wizosky)          875         1       2  28.0   \n",
      "...                                            ...       ...     ...   ...   \n",
      "de Mulder, Mr. Theodore                        287         1       3  30.0   \n",
      "de Pelsmaeker, Mr. Alfons                      283         0       3  16.0   \n",
      "del Carlo, Mr. Sebastiano                      362         0       2  29.0   \n",
      "van Billiard, Mr. Austin Blyler                154         0       3  40.5   \n",
      "van Melkebeke, Mr. Philemon                    869         0       3   NaN   \n",
      "\n",
      "                                       SibSp  Parch     Fare  NameLength  \n",
      "Name                                                                      \n",
      "Abbing, Mr. Anthony                        0      0   7.5500          19  \n",
      "Abbott, Mr. Rossmore Edward                1      1  20.2500          27  \n",
      "Abbott, Mrs. Stanton (Rosa Hunt)           1      1  20.2500          32  \n",
      "Abelson, Mr. Samuel                        1      0  24.0000          19  \n",
      "Abelson, Mrs. Samuel (Hannah Wizosky)      1      0  24.0000          37  \n",
      "...                                      ...    ...      ...         ...  \n",
      "de Mulder, Mr. Theodore                    0      0   9.5000          23  \n",
      "de Pelsmaeker, Mr. Alfons                  0      0   9.5000          25  \n",
      "del Carlo, Mr. Sebastiano                  1      0  27.7208          25  \n",
      "van Billiard, Mr. Austin Blyler            0      2  14.5000          31  \n",
      "van Melkebeke, Mr. Philemon                0      0   9.5000          27  \n",
      "\n",
      "[891 rows x 8 columns]\n",
      "************* Sex **************\n",
      "2\n",
      "        PassengerId  Survived    Pclass        Age     SibSp     Parch  \\\n",
      "Sex                                                                      \n",
      "female   431.028662  0.742038  2.159236  27.915709  0.694268  0.649682   \n",
      "male     454.147314  0.188908  2.389948  30.726645  0.429809  0.235702   \n",
      "\n",
      "             Fare  NameLength  \n",
      "Sex                            \n",
      "female  44.479818   32.608280  \n",
      "male    25.523893   23.894281  \n",
      "************* Age **************\n",
      "89\n",
      "       PassengerId  Survived  Pclass  SibSp  Parch      Fare  NameLength\n",
      "Age                                                                     \n",
      "0.42         804.0       1.0     3.0    0.0    1.0    8.5167        31.0\n",
      "0.67         756.0       1.0     2.0    1.0    1.0   14.5000        25.0\n",
      "0.75         557.5       1.0     3.0    2.0    1.0   19.2583        25.5\n",
      "0.83         455.5       1.0     2.0    0.5    1.5   23.8750        30.0\n",
      "0.92         306.0       1.0     1.0    1.0    2.0  151.5500        30.0\n",
      "...            ...       ...     ...    ...    ...       ...         ...\n",
      "70.00        709.5       0.0     1.5    0.5    0.5   40.7500        27.5\n",
      "70.50        117.0       0.0     3.0    0.0    0.0    7.7500        20.0\n",
      "71.00        295.5       0.0     1.0    0.0    0.0   42.0792        24.0\n",
      "74.00        852.0       0.0     3.0    0.0    0.0    7.7750        19.0\n",
      "80.00        631.0       1.0     1.0    0.0    0.0   30.0000        36.0\n",
      "\n",
      "[88 rows x 7 columns]\n",
      "************* SibSp **************\n",
      "7\n",
      "       PassengerId  Survived    Pclass        Age     Parch       Fare  \\\n",
      "SibSp                                                                    \n",
      "0       455.370066  0.345395  2.351974  31.397558  0.185855  25.692028   \n",
      "1       439.727273  0.535885  2.057416  30.089727  0.655502  44.147370   \n",
      "2       412.428571  0.464286  2.357143  22.620000  0.642857  51.753718   \n",
      "3       321.562500  0.250000  2.562500  13.916667  1.312500  68.908862   \n",
      "4       381.611111  0.166667  3.000000   7.055556  1.500000  31.855556   \n",
      "5       336.800000  0.000000  3.000000  10.200000  2.000000  46.900000   \n",
      "8       481.714286  0.000000  3.000000        NaN  2.000000  69.550000   \n",
      "\n",
      "       NameLength  \n",
      "SibSp              \n",
      "0       24.753289  \n",
      "1       32.947368  \n",
      "2       28.142857  \n",
      "3       28.562500  \n",
      "4       28.888889  \n",
      "5       29.600000  \n",
      "8       25.285714  \n",
      "************* Parch **************\n",
      "7\n",
      "       PassengerId  Survived    Pclass        Age     SibSp       Fare  \\\n",
      "Parch                                                                    \n",
      "0       445.255162  0.343658  2.321534  32.178503  0.237463  25.586774   \n",
      "1       465.110169  0.550847  2.203390  24.422000  1.084746  46.778180   \n",
      "2       416.662500  0.500000  2.275000  17.216912  2.062500  64.337604   \n",
      "3       579.200000  0.600000  2.600000  33.200000  1.000000  25.951660   \n",
      "4       384.000000  0.000000  2.500000  44.500000  0.750000  84.968750   \n",
      "5       435.200000  0.200000  3.000000  39.200000  0.600000  32.550000   \n",
      "6       679.000000  0.000000  3.000000  43.000000  1.000000  46.900000   \n",
      "\n",
      "       NameLength  \n",
      "Parch              \n",
      "0       25.690265  \n",
      "1       30.432203  \n",
      "2       30.825000  \n",
      "3       34.400000  \n",
      "4       31.250000  \n",
      "5       43.000000  \n",
      "6       39.000000  \n",
      "************* Ticket **************\n",
      "681\n",
      "             PassengerId  Survived  Pclass        Age     SibSp     Parch  \\\n",
      "Ticket                                                                      \n",
      "110152        507.666667  1.000000     1.0  26.333333  0.000000  0.000000   \n",
      "110413        469.333333  0.666667     1.0  36.333333  0.666667  1.333333   \n",
      "110465        293.500000  0.000000     1.0  47.000000  0.000000  0.000000   \n",
      "110564        431.000000  1.000000     1.0  28.000000  0.000000  0.000000   \n",
      "110813        367.000000  1.000000     1.0  60.000000  1.000000  0.000000   \n",
      "...                  ...       ...     ...        ...       ...       ...   \n",
      "W./C. 6608    352.250000  0.000000     3.0  23.500000  1.500000  2.500000   \n",
      "W./C. 6609    236.000000  0.000000     3.0        NaN  0.000000  0.000000   \n",
      "W.E.P. 5734    93.000000  0.000000     1.0  46.000000  1.000000  0.000000   \n",
      "W/C 14208     220.000000  0.000000     2.0  30.000000  0.000000  0.000000   \n",
      "WE/P 5735     643.500000  0.500000     1.0  53.000000  0.500000  1.500000   \n",
      "\n",
      "               Fare  NameLength  \n",
      "Ticket                           \n",
      "110152       86.500   32.333333  \n",
      "110413       79.650   24.666667  \n",
      "110465       52.000   28.500000  \n",
      "110564       26.550   41.000000  \n",
      "110813       75.250   48.000000  \n",
      "...             ...         ...  \n",
      "W./C. 6608   34.375   32.250000  \n",
      "W./C. 6609    7.550   28.000000  \n",
      "W.E.P. 5734  61.175   27.000000  \n",
      "W/C 14208    10.500   18.000000  \n",
      "WE/P 5735    71.000   25.500000  \n",
      "\n",
      "[681 rows x 8 columns]\n",
      "************* Fare **************\n",
      "248\n",
      "          PassengerId  Survived    Pclass        Age  SibSp     Parch  \\\n",
      "Fare                                                                    \n",
      "0.0000         516.40  0.066667  1.933333  35.142857   0.00  0.000000   \n",
      "4.0125         379.00  0.000000  3.000000  20.000000   0.00  0.000000   \n",
      "5.0000         873.00  0.000000  1.000000  33.000000   0.00  0.000000   \n",
      "6.2375         327.00  0.000000  3.000000  61.000000   0.00  0.000000   \n",
      "6.4375         844.00  0.000000  3.000000  34.500000   0.00  0.000000   \n",
      "...               ...       ...       ...        ...    ...       ...   \n",
      "227.5250       589.25  0.750000  1.000000  32.666667   0.25  0.000000   \n",
      "247.5208       209.50  0.500000  1.000000  37.000000   0.00  1.000000   \n",
      "262.3750       527.50  1.000000  1.000000  19.500000   2.00  2.000000   \n",
      "263.0000       224.50  0.500000  1.000000  32.500000   2.50  2.500000   \n",
      "512.3292       559.00  1.000000  1.000000  35.333333   0.00  0.333333   \n",
      "\n",
      "          NameLength  \n",
      "Fare                  \n",
      "0.0000         24.80  \n",
      "4.0125         19.00  \n",
      "5.0000         24.00  \n",
      "6.2375         25.00  \n",
      "6.4375         26.00  \n",
      "...              ...  \n",
      "227.5250       29.50  \n",
      "247.5208       35.50  \n",
      "262.3750       31.50  \n",
      "263.0000       25.75  \n",
      "512.3292       24.00  \n",
      "\n",
      "[248 rows x 7 columns]\n",
      "************* Cabin **************\n",
      "148\n",
      "       PassengerId  Survived  Pclass    Age  SibSp  Parch       Fare  \\\n",
      "Cabin                                                                  \n",
      "A10          584.0       0.0     1.0  36.00    0.0   0.00  40.125000   \n",
      "A14          476.0       0.0     1.0    NaN    0.0   0.00  52.000000   \n",
      "A16          557.0       1.0     1.0  48.00    1.0   0.00  39.600000   \n",
      "A19          285.0       0.0     1.0    NaN    0.0   0.00  26.000000   \n",
      "A20          600.0       1.0     1.0  49.00    1.0   0.00  56.929200   \n",
      "...            ...       ...     ...    ...    ...    ...        ...   \n",
      "F33          310.0       1.0     2.0  29.00    0.0   0.00  11.333333   \n",
      "F38          777.0       0.0     3.0    NaN    0.0   0.00   7.750000   \n",
      "F4           401.5       1.0     2.0   2.50    2.0   1.00  39.000000   \n",
      "G6           216.0       0.5     3.0  14.75    0.5   1.25  13.581250   \n",
      "T            340.0       0.0     1.0  45.00    0.0   0.00  35.500000   \n",
      "\n",
      "       NameLength  \n",
      "Cabin              \n",
      "A10     19.000000  \n",
      "A14     27.000000  \n",
      "A16     65.000000  \n",
      "A19     26.000000  \n",
      "A20     44.000000  \n",
      "...           ...  \n",
      "F33     28.333333  \n",
      "F38     16.000000  \n",
      "F4      26.000000  \n",
      "G6      37.500000  \n",
      "T       28.000000  \n",
      "\n",
      "[147 rows x 8 columns]\n",
      "************* Embarked **************\n",
      "4\n",
      "          PassengerId  Survived    Pclass        Age     SibSp     Parch  \\\n",
      "Embarked                                                                   \n",
      "C          445.357143  0.553571  1.886905  30.814769  0.386905  0.363095   \n",
      "Q          417.896104  0.389610  2.909091  28.089286  0.428571  0.168831   \n",
      "S          449.527950  0.336957  2.350932  29.445397  0.571429  0.413043   \n",
      "\n",
      "               Fare  NameLength  \n",
      "Embarked                         \n",
      "C         59.954144   27.494048  \n",
      "Q         13.276030   22.714286  \n",
      "S         27.079812   27.326087  \n",
      "************* NameLength **************\n",
      "50\n",
      "            PassengerId  Survived    Pclass        Age     SibSp     Parch  \\\n",
      "NameLength                                                                   \n",
      "12           760.000000  0.500000  3.000000        NaN  0.000000  0.000000   \n",
      "13           122.500000  0.500000  3.000000  30.000000  0.000000  0.000000   \n",
      "14           518.000000  0.333333  3.000000  25.000000  0.000000  0.000000   \n",
      "15           499.466667  0.133333  2.866667  32.818182  0.000000  0.000000   \n",
      "16           467.923077  0.230769  2.653846  33.694444  0.076923  0.076923   \n",
      "17           476.642857  0.214286  2.523810  34.269231  0.261905  0.190476   \n",
      "18           514.640000  0.200000  2.620000  30.227273  0.420000  0.300000   \n",
      "19           431.765625  0.234375  2.546875  29.464286  0.281250  0.140625   \n",
      "20           395.692308  0.282051  2.487179  29.140625  0.333333  0.076923   \n",
      "21           422.125000  0.325000  2.125000  29.103448  0.125000  0.200000   \n",
      "22           483.078947  0.315789  2.473684  26.411290  0.289474  0.263158   \n",
      "23           420.358974  0.282051  2.205128  35.030303  0.615385  0.205128   \n",
      "24           419.279070  0.372093  2.395349  29.677419  0.651163  0.302326   \n",
      "25           446.672727  0.327273  2.072727  29.801346  0.472727  0.309091   \n",
      "26           411.979592  0.224490  2.306122  29.700000  0.632653  0.387755   \n",
      "27           447.420000  0.360000  2.180000  29.295455  0.560000  0.440000   \n",
      "28           458.116279  0.372093  2.186047  26.307692  0.720930  0.372093   \n",
      "29           483.500000  0.500000  2.250000  25.233846  0.625000  0.406250   \n",
      "30           328.324324  0.432432  2.243243  25.664000  0.810811  0.432432   \n",
      "31           437.333333  0.400000  2.466667  22.901786  0.566667  0.466667   \n",
      "32           440.391304  0.565217  2.521739  33.055556  0.260870  0.391304   \n",
      "33           442.954545  0.545455  2.181818  26.764706  1.136364  0.545455   \n",
      "34           478.428571  0.428571  2.428571  21.428571  1.571429  1.000000   \n",
      "35           446.833333  1.000000  1.833333  22.666667  0.500000  0.666667   \n",
      "36           526.111111  0.333333  2.222222  34.875000  0.888889  1.111111   \n",
      "37           529.900000  0.700000  2.200000  24.444444  1.300000  1.500000   \n",
      "38           436.222222  0.444444  2.000000  27.166667  0.333333  1.333333   \n",
      "39           590.111111  0.444444  2.666667  26.125000  1.000000  1.666667   \n",
      "40           341.571429  0.428571  2.428571  34.300000  0.428571  1.142857   \n",
      "41           525.125000  1.000000  1.375000  38.142857  0.500000  0.375000   \n",
      "42           535.000000  0.200000  2.000000  27.200000  0.600000  0.400000   \n",
      "43           593.000000  0.800000  2.200000  32.600000  1.200000  1.200000   \n",
      "44           468.375000  1.000000  1.750000  32.857143  0.625000  0.125000   \n",
      "45           440.333333  0.777778  1.555556  42.000000  0.555556  0.444444   \n",
      "46           366.428571  0.571429  2.000000  40.666667  0.857143  0.142857   \n",
      "47           377.363636  0.727273  2.000000  33.900000  0.727273  1.000000   \n",
      "48           650.000000  1.000000  1.000000  43.666667  0.666667  0.333333   \n",
      "49           504.400000  1.000000  2.200000  22.500000  0.800000  0.600000   \n",
      "50           343.000000  1.000000  1.500000  40.333333  0.750000  0.250000   \n",
      "51           428.857143  1.000000  1.714286  33.285714  0.857143  0.571429   \n",
      "52           511.000000  0.750000  2.250000  32.250000  1.000000  0.500000   \n",
      "53           694.500000  1.000000  1.500000  32.500000  0.500000  1.500000   \n",
      "54           424.000000  0.000000  3.000000  28.000000  1.000000  1.000000   \n",
      "55            52.500000  0.500000  3.000000  32.000000  2.000000  0.000000   \n",
      "56           374.000000  0.666667  1.333333  33.333333  0.666667  0.333333   \n",
      "57           318.500000  0.500000  3.000000  38.500000  1.000000  5.000000   \n",
      "61           671.000000  1.000000  2.000000  40.000000  1.000000  1.000000   \n",
      "65           557.000000  1.000000  1.000000  48.000000  1.000000  0.000000   \n",
      "67           428.000000  1.000000  2.000000  19.000000  0.000000  0.000000   \n",
      "82           308.000000  1.000000  1.000000  17.000000  1.000000  0.000000   \n",
      "\n",
      "                  Fare  \n",
      "NameLength              \n",
      "12           56.495800  \n",
      "13           56.495800  \n",
      "14           23.591667  \n",
      "15           15.626100  \n",
      "16           29.052723  \n",
      "17           21.680550  \n",
      "18           21.128164  \n",
      "19           22.648567  \n",
      "20           24.525426  \n",
      "21           26.451563  \n",
      "22           34.278508  \n",
      "23           29.705449  \n",
      "24           34.256681  \n",
      "25           35.479316  \n",
      "26           32.594643  \n",
      "27           29.913338  \n",
      "28           27.540988  \n",
      "29           38.574350  \n",
      "30           41.617457  \n",
      "31           19.572640  \n",
      "32           18.050722  \n",
      "33           36.903409  \n",
      "34           96.186314  \n",
      "35           40.682633  \n",
      "36           33.783333  \n",
      "37           51.061660  \n",
      "38           34.741667  \n",
      "39           30.764356  \n",
      "40           29.052986  \n",
      "41           50.346350  \n",
      "42           60.975840  \n",
      "43           24.115000  \n",
      "44           43.449488  \n",
      "45           48.555556  \n",
      "46           47.074400  \n",
      "47           67.790536  \n",
      "48           59.102800  \n",
      "49           64.400820  \n",
      "50           67.225000  \n",
      "51           35.416071  \n",
      "52           44.220825  \n",
      "53          126.458350  \n",
      "54           14.400000  \n",
      "55           16.925000  \n",
      "56           80.666667  \n",
      "57           31.331250  \n",
      "61           39.000000  \n",
      "65           39.600000  \n",
      "67           26.000000  \n",
      "82          108.900000  \n"
     ]
    }
   ],
   "source": [
    "for (columnName, columnData) in dataset.iteritems():\n",
    "    print('*************', columnName, '**************')\n",
    "    print(len(dataset[columnName].unique()))\n",
    "    print(dataset.groupby(columnName).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "survival_stats(dataset.drop('Survived', axis=1), dataset['Survived'], 'Sex')\n",
    "survival_stats(dataset.drop('Survived', axis=1), dataset['Survived'], 'Pclass')\n",
    "survival_stats(dataset.drop('Survived', axis=1), dataset['Survived'], 'SibSp')\n",
    "survival_stats(dataset.drop('Survived', axis=1), dataset['Survived'], 'Parch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('PassengerId', axis=1, inplace=True)\n",
    "#dataset.drop('Cabin', axis=1, inplace=True)\n",
    "dataset.drop('Name', axis=1, inplace=True)\n",
    "dataset.drop('Ticket', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NameLength</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.591667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>32.818182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.653846</td>\n",
       "      <td>33.694444</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>29.052723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>34.269231</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>21.680550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>30.227273</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>21.128164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>29.464286</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>22.648567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.282051</td>\n",
       "      <td>2.487179</td>\n",
       "      <td>29.140625</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>24.525426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.325000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>29.103448</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>26.451563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>2.473684</td>\n",
       "      <td>26.411290</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>34.278508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.282051</td>\n",
       "      <td>2.205128</td>\n",
       "      <td>35.030303</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>29.705449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.372093</td>\n",
       "      <td>2.395349</td>\n",
       "      <td>29.677419</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>34.256681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.327273</td>\n",
       "      <td>2.072727</td>\n",
       "      <td>29.801346</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>35.479316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.224490</td>\n",
       "      <td>2.306122</td>\n",
       "      <td>29.700000</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>32.594643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>2.180000</td>\n",
       "      <td>29.295455</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>29.913338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.372093</td>\n",
       "      <td>2.186047</td>\n",
       "      <td>26.307692</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>27.540988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>25.233846</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>38.574350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.432432</td>\n",
       "      <td>2.243243</td>\n",
       "      <td>25.664000</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>41.617457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>22.901786</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>19.572640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>2.521739</td>\n",
       "      <td>33.055556</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>18.050722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>26.764706</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>36.903409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.186314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>22.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>40.682633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>34.875000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>33.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>24.444444</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>51.061660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>27.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>34.741667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>26.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>30.764356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>34.300000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>29.052986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>38.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>50.346350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>60.975840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>32.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>24.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>32.857143</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>43.449488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>48.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>47.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>33.900000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.790536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>59.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.400820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>40.333333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>67.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>33.285714</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>35.416071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>32.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>44.220825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>126.458350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>80.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>31.331250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Survived    Pclass        Age     SibSp     Parch        Fare\n",
       "NameLength                                                               \n",
       "12          0.500000  3.000000        NaN  0.000000  0.000000   56.495800\n",
       "13          0.500000  3.000000  30.000000  0.000000  0.000000   56.495800\n",
       "14          0.333333  3.000000  25.000000  0.000000  0.000000   23.591667\n",
       "15          0.133333  2.866667  32.818182  0.000000  0.000000   15.626100\n",
       "16          0.230769  2.653846  33.694444  0.076923  0.076923   29.052723\n",
       "17          0.214286  2.523810  34.269231  0.261905  0.190476   21.680550\n",
       "18          0.200000  2.620000  30.227273  0.420000  0.300000   21.128164\n",
       "19          0.234375  2.546875  29.464286  0.281250  0.140625   22.648567\n",
       "20          0.282051  2.487179  29.140625  0.333333  0.076923   24.525426\n",
       "21          0.325000  2.125000  29.103448  0.125000  0.200000   26.451563\n",
       "22          0.315789  2.473684  26.411290  0.289474  0.263158   34.278508\n",
       "23          0.282051  2.205128  35.030303  0.615385  0.205128   29.705449\n",
       "24          0.372093  2.395349  29.677419  0.651163  0.302326   34.256681\n",
       "25          0.327273  2.072727  29.801346  0.472727  0.309091   35.479316\n",
       "26          0.224490  2.306122  29.700000  0.632653  0.387755   32.594643\n",
       "27          0.360000  2.180000  29.295455  0.560000  0.440000   29.913338\n",
       "28          0.372093  2.186047  26.307692  0.720930  0.372093   27.540988\n",
       "29          0.500000  2.250000  25.233846  0.625000  0.406250   38.574350\n",
       "30          0.432432  2.243243  25.664000  0.810811  0.432432   41.617457\n",
       "31          0.400000  2.466667  22.901786  0.566667  0.466667   19.572640\n",
       "32          0.565217  2.521739  33.055556  0.260870  0.391304   18.050722\n",
       "33          0.545455  2.181818  26.764706  1.136364  0.545455   36.903409\n",
       "34          0.428571  2.428571  21.428571  1.571429  1.000000   96.186314\n",
       "35          1.000000  1.833333  22.666667  0.500000  0.666667   40.682633\n",
       "36          0.333333  2.222222  34.875000  0.888889  1.111111   33.783333\n",
       "37          0.700000  2.200000  24.444444  1.300000  1.500000   51.061660\n",
       "38          0.444444  2.000000  27.166667  0.333333  1.333333   34.741667\n",
       "39          0.444444  2.666667  26.125000  1.000000  1.666667   30.764356\n",
       "40          0.428571  2.428571  34.300000  0.428571  1.142857   29.052986\n",
       "41          1.000000  1.375000  38.142857  0.500000  0.375000   50.346350\n",
       "42          0.200000  2.000000  27.200000  0.600000  0.400000   60.975840\n",
       "43          0.800000  2.200000  32.600000  1.200000  1.200000   24.115000\n",
       "44          1.000000  1.750000  32.857143  0.625000  0.125000   43.449488\n",
       "45          0.777778  1.555556  42.000000  0.555556  0.444444   48.555556\n",
       "46          0.571429  2.000000  40.666667  0.857143  0.142857   47.074400\n",
       "47          0.727273  2.000000  33.900000  0.727273  1.000000   67.790536\n",
       "48          1.000000  1.000000  43.666667  0.666667  0.333333   59.102800\n",
       "49          1.000000  2.200000  22.500000  0.800000  0.600000   64.400820\n",
       "50          1.000000  1.500000  40.333333  0.750000  0.250000   67.225000\n",
       "51          1.000000  1.714286  33.285714  0.857143  0.571429   35.416071\n",
       "52          0.750000  2.250000  32.250000  1.000000  0.500000   44.220825\n",
       "53          1.000000  1.500000  32.500000  0.500000  1.500000  126.458350\n",
       "54          0.000000  3.000000  28.000000  1.000000  1.000000   14.400000\n",
       "55          0.500000  3.000000  32.000000  2.000000  0.000000   16.925000\n",
       "56          0.666667  1.333333  33.333333  0.666667  0.333333   80.666667\n",
       "57          0.500000  3.000000  38.500000  1.000000  5.000000   31.331250\n",
       "61          1.000000  2.000000  40.000000  1.000000  1.000000   39.000000\n",
       "65          1.000000  1.000000  48.000000  1.000000  0.000000   39.600000\n",
       "67          1.000000  2.000000  19.000000  0.000000  0.000000   26.000000\n",
       "82          1.000000  1.000000  17.000000  1.000000  0.000000  108.900000"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('NameLength').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked  \\\n",
       "0         0       3    male  22.0      1      0   7.2500   NaN        S   \n",
       "1         1       1  female  38.0      1      0  71.2833   C85        C   \n",
       "2         1       3  female  26.0      0      0   7.9250   NaN        S   \n",
       "3         1       1  female  35.0      1      0  53.1000  C123        S   \n",
       "4         0       3    male  35.0      0      0   8.0500   NaN        S   \n",
       "5         0       3    male   NaN      0      0   8.4583   NaN        Q   \n",
       "6         0       1    male  54.0      0      0  51.8625   E46        S   \n",
       "7         0       3    male   2.0      3      1  21.0750   NaN        S   \n",
       "8         1       3  female  27.0      0      2  11.1333   NaN        S   \n",
       "9         1       2  female  14.0      1      0  30.0708   NaN        C   \n",
       "\n",
       "   NameLength  \n",
       "0          23  \n",
       "1          51  \n",
       "2          22  \n",
       "3          44  \n",
       "4          24  \n",
       "5          16  \n",
       "6          23  \n",
       "7          30  \n",
       "8          49  \n",
       "9          35  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see graphically which is the disease distribution inside dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4c51d790>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['Survived'].value_counts()[:20].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- One hot encoding\n",
    "- To categorical\n",
    "- Data regularizarion\n",
    "Fare is some kind of cathegorical value. Let's split into 4 different categories.\n",
    "- 0 to 8\n",
    "- 8.1 to 14.5\n",
    "- 14.6 to 31\n",
    "- More than 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)\n",
    "dataset.to_pickle('data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Cabin  Embarked  \\\n",
       "0         0       3    1  22.0      1      0   7.2500      0       0.0   \n",
       "1         1       1    0  38.0      1      0  71.2833      1       1.0   \n",
       "2         1       3    0  26.0      0      0   7.9250      0       0.0   \n",
       "3         1       1    0  35.0      1      0  53.1000      1       0.0   \n",
       "4         0       3    1  35.0      0      0   8.0500      0       0.0   \n",
       "5         0       3    1   NaN      0      0   8.4583      0       2.0   \n",
       "6         0       1    1  54.0      0      0  51.8625      1       0.0   \n",
       "7         0       3    1   2.0      3      1  21.0750      0       0.0   \n",
       "8         1       3    0  27.0      0      2  11.1333      0       0.0   \n",
       "9         1       2    0  14.0      1      0  30.0708      0       1.0   \n",
       "\n",
       "   NameLength  \n",
       "0          23  \n",
       "1          51  \n",
       "2          22  \n",
       "3          44  \n",
       "4          24  \n",
       "5          16  \n",
       "6          23  \n",
       "7          30  \n",
       "8          49  \n",
       "9          35  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle('data/dataset.pkl')\n",
    "dataset['Sex'].replace(['female','male'],[0,1],inplace=True)\n",
    "dataset['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "dataset.loc[dataset['Cabin'].notnull(), 'Cabin'] = 1\n",
    "dataset['Cabin'].fillna(0, inplace=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n"
     ]
    }
   ],
   "source": [
    "dataset\n",
    "dataset = dataset.dropna()\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "885    0\n",
      "886    0\n",
      "887    1\n",
      "889    1\n",
      "890    0\n",
      "Name: Survived, Length: 712, dtype: int64\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Survived'])\n",
    "# Split the data into features and target label\n",
    "diagnosis_raw = dataset['Survived']\n",
    "features_raw = dataset.drop('Survived', axis = 1)\n",
    "# Get number of deseases. We are going to use it for, for instance, set the output layer size in the prediction section\n",
    "number_of_diseases = len(diagnosis_raw.unique())\n",
    "print(len(diagnosis_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let going to log transform to numerical values to ensure all values are in range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688670</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  NameLength\n",
       "0       3    1  0.688670      1      0  0.338125      0       0.0    0.302844\n",
       "1       1    0  0.819257      1      0  0.685892      1       1.0    0.737273\n",
       "2       3    0  0.728321      0      0  0.350727      0       0.0    0.278931\n",
       "3       1    0  0.799463      1      0  0.639463      1       0.0    0.656038\n",
       "4       3    1  0.799463      0      0  0.352955      0       0.0    0.325780"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log transform numerical value colums\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[['Age','Fare','NameLength']] = features_raw[['Age','Fare','NameLength']].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[['Age','Fare','NameLength']] = scaler.fit_transform(features_log_transformed[['Age','Fare','NameLength']])\n",
    "\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Age':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.460439</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.601092</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.544984</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399934</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.502583</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390821</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.171584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.805569</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.805569</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.453029</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.451521</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.389456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.707770</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414494</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.529578</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427563</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565039</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.509229</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.439173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.492161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.361012</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.582879</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399934</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "7         3    1  0.184964      3      1  0.495832      0       0.0   \n",
       "10        3    0  0.311287      1      1  0.460439      1       0.0   \n",
       "16        3    1  0.184964      4      1  0.545650      0       2.0   \n",
       "43        2    0  0.256106      1      2  0.601092      0       1.0   \n",
       "50        3    1  0.427516      4      1  0.593810      0       0.0   \n",
       "58        2    0  0.356374      1      2  0.538164      0       0.0   \n",
       "63        3    1  0.311287      3      2  0.538998      0       0.0   \n",
       "78        2    1  0.062728      0      2  0.544984      0       0.0   \n",
       "119       3    0  0.184964      4      2  0.556696      0       0.0   \n",
       "164       3    1  0.084695      4      1  0.593810      0       0.0   \n",
       "171       3    1  0.311287      4      1  0.545650      0       2.0   \n",
       "172       3    0  0.084695      1      1  0.399934      0       0.0   \n",
       "183       2    1  0.084695      2      1  0.591080      1       0.0   \n",
       "184       3    0  0.311287      0      2  0.502583      0       0.0   \n",
       "193       2    1  0.256106      1      1  0.528101      1       0.0   \n",
       "205       3    0  0.184964      0      1  0.390821      1       0.0   \n",
       "233       3    0  0.356374      4      2  0.557253      0       0.0   \n",
       "261       3    1  0.256106      4      2  0.557253      0       0.0   \n",
       "278       3    1  0.427516      4      1  0.545650      0       2.0   \n",
       "297       1    0  0.184964      1      2  0.805569      1       0.0   \n",
       "305       1    1  0.074600      1      2  0.805569      1       0.0   \n",
       "340       2    1  0.184964      1      1  0.528101      1       0.0   \n",
       "348       3    1  0.256106      1      1  0.453029      0       0.0   \n",
       "374       3    0  0.256106      3      1  0.495832      0       0.0   \n",
       "381       3    0  0.084695      0      2  0.451521      0       1.0   \n",
       "386       3    1  0.084695      5      2  0.619959      0       0.0   \n",
       "407       2    1  0.256106      1      1  0.477999      0       0.0   \n",
       "445       1    1  0.311287      0      2  0.707770      1       0.0   \n",
       "448       3    0  0.356374      2      1  0.482071      0       1.0   \n",
       "469       3    0  0.051674      2      1  0.482071      0       1.0   \n",
       "479       3    0  0.184964      0      1  0.414494      0       0.0   \n",
       "530       2    0  0.184964      1      1  0.528101      0       0.0   \n",
       "535       2    0  0.427516      0      2  0.529578      0       0.0   \n",
       "618       2    0  0.311287      2      1  0.591080      1       0.0   \n",
       "642       3    0  0.184964      3      2  0.538998      0       0.0   \n",
       "644       3    0  0.051674      2      1  0.482071      0       1.0   \n",
       "691       3    0  0.311287      0      1  0.427563      0       1.0   \n",
       "720       2    0  0.394494      0      1  0.565039      0       0.0   \n",
       "750       2    0  0.311287      1      1  0.509229      0       0.0   \n",
       "751       3    1  0.394494      0      1  0.416739      1       0.0   \n",
       "755       2    1  0.040103      1      1  0.439173      0       0.0   \n",
       "777       3    0  0.356374      0      0  0.416739      0       0.0   \n",
       "788       3    1  0.084695      1      2  0.492161      0       0.0   \n",
       "803       3    1  0.000000      0      1  0.361012      0       1.0   \n",
       "813       3    0  0.394494      4      2  0.556696      0       0.0   \n",
       "824       3    1  0.184964      4      1  0.593810      0       0.0   \n",
       "827       2    1  0.084695      0      2  0.582879      0       1.0   \n",
       "831       2    1  0.062728      1      1  0.477999      0       0.0   \n",
       "850       3    1  0.311287      4      2  0.556696      0       0.0   \n",
       "869       3    1  0.311287      1      1  0.399934      0       0.0   \n",
       "\n",
       "     NameLength  \n",
       "7      0.446644  \n",
       "10     0.464483  \n",
       "16     0.227817  \n",
       "43     0.603733  \n",
       "50     0.369022  \n",
       "58     0.409173  \n",
       "63     0.253955  \n",
       "78     0.428221  \n",
       "119    0.498546  \n",
       "164    0.409173  \n",
       "171    0.227817  \n",
       "172    0.409173  \n",
       "183    0.347817  \n",
       "184    0.530661  \n",
       "193    0.369022  \n",
       "205    0.369022  \n",
       "233    0.446644  \n",
       "261    0.498546  \n",
       "278    0.171584  \n",
       "297    0.409173  \n",
       "305    0.446644  \n",
       "340    0.446644  \n",
       "348    0.575634  \n",
       "374    0.369022  \n",
       "381    0.389456  \n",
       "386    0.464483  \n",
       "407    0.446644  \n",
       "445    0.347817  \n",
       "448    0.446644  \n",
       "469    0.428221  \n",
       "479    0.325780  \n",
       "530    0.325780  \n",
       "535    0.278931  \n",
       "618    0.389456  \n",
       "642    0.428221  \n",
       "644    0.278931  \n",
       "691    0.171584  \n",
       "720    0.498546  \n",
       "750    0.141205  \n",
       "751    0.200404  \n",
       "755    0.347817  \n",
       "777    0.428221  \n",
       "788    0.369022  \n",
       "803    0.464483  \n",
       "813    0.514833  \n",
       "824    0.409173  \n",
       "827    0.253955  \n",
       "831    0.464483  \n",
       "850    0.589860  \n",
       "869    0.464483  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'SibSp':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456643</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527784</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628053</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613918</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.785328</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.699194</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482697</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613918</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.171584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482697</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.482697</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.527784</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.482697</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582966</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582966</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762485</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456643</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.347817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506267</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "7         3    1  0.184964      3      1  0.495832      0       0.0   \n",
       "16        3    1  0.184964      4      1  0.545650      0       2.0   \n",
       "24        3    0  0.456643      3      1  0.495832      0       0.0   \n",
       "27        1    1  0.654108      3      2  0.893450      1       0.0   \n",
       "50        3    1  0.427516      4      1  0.593810      0       0.0   \n",
       "59        3    1  0.527784      5      2  0.619959      0       0.0   \n",
       "63        3    1  0.311287      3      2  0.538998      0       0.0   \n",
       "68        3    0  0.628053      4      2  0.350727      0       0.0   \n",
       "71        3    0  0.613918      5      2  0.619959      0       0.0   \n",
       "85        3    0  0.785328      3      0  0.452554      0       0.0   \n",
       "88        1    0  0.699194      3      2  0.893450      1       0.0   \n",
       "119       3    0  0.184964      4      2  0.556696      0       0.0   \n",
       "164       3    1  0.084695      4      1  0.593810      0       0.0   \n",
       "171       3    1  0.311287      4      1  0.545650      0       2.0   \n",
       "182       3    1  0.482697      4      2  0.557253      0       0.0   \n",
       "233       3    0  0.356374      4      2  0.557253      0       0.0   \n",
       "261       3    1  0.256106      4      2  0.557253      0       0.0   \n",
       "266       3    1  0.613918      4      1  0.593810      0       0.0   \n",
       "278       3    1  0.427516      4      1  0.545650      0       2.0   \n",
       "341       1    0  0.709289      3      2  0.893450      1       0.0   \n",
       "374       3    0  0.256106      3      1  0.495832      0       0.0   \n",
       "386       3    1  0.084695      5      2  0.619959      0       0.0   \n",
       "480       3    1  0.482697      5      2  0.619959      0       0.0   \n",
       "541       3    0  0.482697      4      2  0.556696      0       0.0   \n",
       "542       3    0  0.527784      4      2  0.556696      0       0.0   \n",
       "634       3    0  0.482697      3      2  0.538998      0       0.0   \n",
       "642       3    0  0.184964      3      2  0.538998      0       0.0   \n",
       "683       3    1  0.582966      5      2  0.619959      0       0.0   \n",
       "686       3    1  0.582966      4      1  0.593810      0       0.0   \n",
       "726       2    0  0.762485      3      0  0.495287      0       0.0   \n",
       "787       3    1  0.456643      4      1  0.545650      0       2.0   \n",
       "813       3    0  0.394494      4      2  0.556696      0       0.0   \n",
       "819       3    1  0.506267      3      2  0.538998      0       0.0   \n",
       "824       3    1  0.184964      4      1  0.593810      0       0.0   \n",
       "850       3    1  0.311287      4      2  0.556696      0       0.0   \n",
       "\n",
       "     NameLength  \n",
       "7      0.446644  \n",
       "16     0.227817  \n",
       "24     0.428221  \n",
       "27     0.446644  \n",
       "50     0.369022  \n",
       "59     0.514833  \n",
       "63     0.253955  \n",
       "68     0.464483  \n",
       "71     0.369022  \n",
       "85     0.778912  \n",
       "88     0.369022  \n",
       "119    0.498546  \n",
       "164    0.409173  \n",
       "171    0.227817  \n",
       "182    0.561040  \n",
       "233    0.446644  \n",
       "261    0.498546  \n",
       "266    0.347817  \n",
       "278    0.171584  \n",
       "341    0.446644  \n",
       "374    0.369022  \n",
       "386    0.464483  \n",
       "480    0.446644  \n",
       "541    0.546056  \n",
       "542    0.498546  \n",
       "634    0.171584  \n",
       "642    0.428221  \n",
       "683    0.389456  \n",
       "686    0.325780  \n",
       "726    0.643411  \n",
       "787    0.347817  \n",
       "813    0.514833  \n",
       "819    0.409173  \n",
       "824    0.409173  \n",
       "850    0.589860  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Parch':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613918</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571391</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.860080</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945580</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.754376</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.643411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.837583</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849087</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875704</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571391</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.509229</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.546056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "13        3    1  0.825518      1      5  0.556696      0       0.0   \n",
       "25        3    0  0.819257      1      5  0.557253      0       0.0   \n",
       "86        3    1  0.613918      1      3  0.571391      0       0.0   \n",
       "167       3    0  0.860080      1      4  0.538998      0       0.0   \n",
       "360       3    1  0.831624      1      4  0.538998      0       0.0   \n",
       "437       2    0  0.709289      2      3  0.477999      0       0.0   \n",
       "438       1    1  0.945580      1      4  0.893450      1       0.0   \n",
       "567       3    0  0.754376      0      4  0.495832      0       0.0   \n",
       "610       3    0  0.825518      1      5  0.556696      0       0.0   \n",
       "638       3    0  0.837583      0      5  0.593810      0       0.0   \n",
       "678       3    0  0.849087      1      6  0.619959      0       0.0   \n",
       "736       3    0  0.875704      1      3  0.571391      0       0.0   \n",
       "774       2    0  0.904269      1      3  0.509229      0       0.0   \n",
       "858       3    0  0.709289      0      3  0.482071      0       1.0   \n",
       "885       3    0  0.825518      0      5  0.545650      0       2.0   \n",
       "\n",
       "     NameLength  \n",
       "13     0.389456  \n",
       "25     0.798629  \n",
       "86     0.278931  \n",
       "167    0.692300  \n",
       "360    0.171584  \n",
       "437    0.561040  \n",
       "438    0.141205  \n",
       "567    0.643411  \n",
       "610    0.798629  \n",
       "638    0.575634  \n",
       "678    0.589860  \n",
       "736    0.589860  \n",
       "774    0.561040  \n",
       "858    0.561040  \n",
       "885    0.546056  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Fare':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.699194</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883769</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.885597</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883769</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641423</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893070</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.369022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945580</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.677677</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.893070</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "27        1    1  0.654108      3      2  0.893450      1       0.0   \n",
       "88        1    0  0.699194      3      2  0.893450      1       0.0   \n",
       "118       1    1  0.709289      0      1  0.883769      1       1.0   \n",
       "179       3    1  0.806239      0      0  0.000000      0       0.0   \n",
       "258       1    0  0.799463      0      0  1.000000      0       1.0   \n",
       "263       1    1  0.831624      0      0  0.000000      1       0.0   \n",
       "271       3    1  0.718988      0      0  0.000000      0       0.0   \n",
       "299       1    0  0.885597      0      1  0.883769      1       1.0   \n",
       "302       3    1  0.654108      0      0  0.000000      0       0.0   \n",
       "311       1    0  0.641423      2      2  0.893070      1       1.0   \n",
       "341       1    0  0.709289      3      2  0.893450      1       0.0   \n",
       "438       1    1  0.945580      1      4  0.893450      1       0.0   \n",
       "597       3    1  0.880700      0      0  0.000000      0       0.0   \n",
       "679       1    1  0.806239      0      1  1.000000      1       1.0   \n",
       "737       1    1  0.799463      0      0  1.000000      1       1.0   \n",
       "742       1    0  0.677677      2      2  0.893070      1       1.0   \n",
       "806       1    1  0.825518      0      0  0.000000      1       0.0   \n",
       "822       1    1  0.819257      0      0  0.000000      0       0.0   \n",
       "\n",
       "     NameLength  \n",
       "27     0.446644  \n",
       "88     0.369022  \n",
       "118    0.325780  \n",
       "179    0.200404  \n",
       "258    0.109090  \n",
       "263    0.253955  \n",
       "271    0.409173  \n",
       "299    0.692300  \n",
       "302    0.464483  \n",
       "311    0.369022  \n",
       "341    0.446644  \n",
       "438    0.141205  \n",
       "597    0.200404  \n",
       "679    0.514833  \n",
       "737    0.278931  \n",
       "742    0.561040  \n",
       "806    0.278931  \n",
       "822    0.464483  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Embarked':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.582966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.530661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598926</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352587</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.389456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538001</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598926</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.337639</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.913102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.710264</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.668387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.546056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550238</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.347554</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.200404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "1         1    0  0.819257      1      0  0.685892      1       1.0   \n",
       "9         2    0  0.582966      1      0  0.550603      0       1.0   \n",
       "16        3    1  0.184964      4      1  0.545650      0       2.0   \n",
       "22        3    0  0.598926      0      0  0.352587      0       2.0   \n",
       "30        1    1  0.831624      0      0  0.538001      0       1.0   \n",
       "..      ...  ...       ...    ...    ...       ...    ...       ...   \n",
       "875       3    0  0.598926      0      0  0.337639      0       1.0   \n",
       "879       1    0  0.913102      0      1  0.710264      1       1.0   \n",
       "885       3    0  0.825518      0      5  0.545650      0       2.0   \n",
       "889       1    1  0.728321      0      0  0.550238      1       1.0   \n",
       "890       3    1  0.777946      0      0  0.347554      0       2.0   \n",
       "\n",
       "     NameLength  \n",
       "1      0.737273  \n",
       "9      0.530661  \n",
       "16     0.227817  \n",
       "22     0.389456  \n",
       "30     0.325780  \n",
       "..          ...  \n",
       "875    0.481772  \n",
       "879    0.668387  \n",
       "885    0.546056  \n",
       "889    0.253955  \n",
       "890    0.200404  \n",
       "\n",
       "[158 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'NameLength':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628053</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.753026</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786579</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.788857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875704</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593465</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.785328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716503</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
       "25        3    0  0.819257      1      5  0.557253      0       0.0   \n",
       "41        2    0  0.737315      1      0  0.495287      0       0.0   \n",
       "307       1    0  0.628053      1      0  0.753026      1       1.0   \n",
       "319       1    0  0.831624      1      1  0.786579      1       1.0   \n",
       "427       2    0  0.654108      0      0  0.528101      0       0.0   \n",
       "556       1    0  0.875704      1      0  0.593465      1       1.0   \n",
       "610       3    0  0.825518      1      5  0.556696      0       0.0   \n",
       "670       2    0  0.831624      1      1  0.591080      0       0.0   \n",
       "759       1    0  0.785328      0      0  0.716503      1       0.0   \n",
       "\n",
       "     NameLength  \n",
       "25     0.798629  \n",
       "41     0.788857  \n",
       "307    1.000000  \n",
       "319    0.788857  \n",
       "427    0.888002  \n",
       "556    0.871228  \n",
       "610    0.798629  \n",
       "670    0.836100  \n",
       "759    0.788857  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected in more than one feature: 43\n",
      "Original records: 712\n",
      "[7, 16, 43, 50, 63, 119, 164, 171, 233, 261, 278, 374, 381, 386, 448, 469, 642, 644, 691, 803, 813, 824, 827, 850, 27, 88, 341, 787, 25, 438, 610, 858, 885, 118, 258, 299, 311, 679, 737, 742, 307, 319, 556]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 803 is out of bounds for axis 0 with size 712",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-cad513f52981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Remove the outliers present in more than one feature, if any were specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgood_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutliers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Records once outliers have been removed:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4290\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4291\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpromote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 803 is out of bounds for axis 0 with size 712"
     ]
    }
   ],
   "source": [
    "# For each integer feature find the data points with extreme high or low values\n",
    "# List to store outliers detected in loop\n",
    "dataset = features_log_minmax_transform\n",
    "all_outliers = []\n",
    "for feature in dataset.columns :\n",
    "    \n",
    "    Q1 = np.percentile(dataset[feature], 25)\n",
    "    Q3 = np.percentile(dataset[feature], 75)\n",
    "    step = (Q3 - Q1)*1.5\n",
    "    \n",
    "    feature_outliers = dataset[~((dataset[feature] >= Q1 - step) & (dataset[feature] <= Q3 + step))]\n",
    "    if len(feature_outliers) == 0 :\n",
    "        continue\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(feature_outliers)\n",
    "    \n",
    "    all_outliers.extend(feature_outliers.index.values)\n",
    "    \n",
    "import collections\n",
    "outliers =  [item for item, count in collections.Counter(all_outliers).items() if count > 1]\n",
    "print('Outliers detected in more than one feature:',len(outliers))\n",
    "print('Original records:',len(dataset))\n",
    "print(outliers)\n",
    "dataset.index[-1]\n",
    "# Remove the outliers present in more than one feature, if any were specified\n",
    "good_data = dataset.drop(dataset.index[outliers]).reset_index(drop = True)\n",
    "print('Records once outliers have been removed:',len(good_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to delete outliers in diagnosis_raw in order to keep same amount of records in features and label dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 712\n",
      "Records once outliers have been removed: 712\n"
     ]
    }
   ],
   "source": [
    "#diagnosis = diagnosis_raw.drop(dataset.index[outliers]).reset_index(drop = True)\n",
    "print('Original records:',len(diagnosis_raw))\n",
    "print('Records once outliers have been removed:',len(diagnosis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "At this point, we have available the following variables, that will be used extensively in the next sections:\n",
    "\n",
    "- good_data: Regularized features dataset (without outliers)\n",
    "- diagnosis: Dataset containing Labels corresponding to features dataset,\n",
    "- number_of_diseases: Number of different diseases to diagnose\n",
    "\n",
    "We are going to store these datasets in files on order to use them throughout next sections without needing to execute all previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_log_minmax_transform.to_pickle('data/good_data.pkl')\n",
    "diagnosis_raw.to_pickle('data/diagnosis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the project is to provide doctors a prediction function that helps them to diagnose a TMD disorder based on patient answers to questionaries. Along previous steps, we have declared a set of variables that are going to be useful for the prediction model.\n",
    "\n",
    "- **dataset**: Imported from `data/good_data.pkl` file. Regularized features dataset (without outliers) \n",
    "- **diagnosis**: Imported from `data/diagnosis_raw.pkl` file. Dataset containing Labels corresponding to features dataset.\n",
    "- **number_of_diseases**: Number of different diseases to diagnose\n",
    "- **reduced_dataset**: Imported from `data/good_data_reduced.pkl` file. Regularized features dataset without columns deleted in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n",
      "712\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_pickle('data/good_data.pkl')\n",
    "diagnosis = pd.read_pickle('data/diagnosis.pkl')\n",
    "print(len(dataset))\n",
    "print(len(diagnosis))\n",
    "print(len(dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     0\n",
       "6     0\n",
       "7     0\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label dataset one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply one hot encoding to diagnosis labels (as we are dealing with 11 different labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "#encoder = LabelEncoder()\n",
    "#encoder.fit(diagnosis)\n",
    "#encoded_Y = encoder.transform(diagnosis)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex       Age  SibSp  Parch      Fare  Cabin  Embarked  \\\n",
      "508       3    1  0.745993      0      0  0.506026      0       0.0   \n",
      "376       3    0  0.688670      0      0  0.338125      0       0.0   \n",
      "821       3    1  0.737315      0      0  0.363449      0       0.0   \n",
      "876       3    1  0.666173      0      0  0.381960      0       0.0   \n",
      "308       2    1  0.762485      1      0  0.515770      0       1.0   \n",
      "..      ...  ...       ...    ...    ...       ...    ...       ...   \n",
      "191       2    1  0.654108      0      0  0.422864      0       0.0   \n",
      "833       3    1  0.699194      0      0  0.349451      0       0.0   \n",
      "340       2    1  0.184964      1      1  0.528101      1       0.0   \n",
      "616       3    1  0.792496      1      1  0.438136      0       0.0   \n",
      "57        3    1  0.750220      0      0  0.337721      0       1.0   \n",
      "\n",
      "     NameLength  \n",
      "508    0.325780  \n",
      "376    0.464483  \n",
      "821    0.141205  \n",
      "876    0.428221  \n",
      "308    0.200404  \n",
      "..          ...  \n",
      "191    0.253955  \n",
      "833    0.278931  \n",
      "340    0.446644  \n",
      "616    0.347817  \n",
      "57     0.200404  \n",
      "\n",
      "[569 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, diagnosis, test_size=0.2)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 865\n",
      "Trainable params: 865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "# First Hidden Layer\n",
    "classifier.add(Dense(32, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\n",
    "classifier.add(Dropout(0.2))\n",
    "# Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "classifier.compile(optimizer ='nadam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/200\n",
      "426/426 [==============================] - 0s 362us/step - loss: 1.5182 - acc: 0.7981 - val_loss: 0.4136 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83217, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 2/200\n",
      "426/426 [==============================] - 0s 315us/step - loss: 1.5311 - acc: 0.7864 - val_loss: 0.4205 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.83217\n",
      "Epoch 3/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.6254 - acc: 0.7817 - val_loss: 0.4201 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.83217 to 0.83916, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 4/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.7958 - acc: 0.7911 - val_loss: 0.4248 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.83916\n",
      "Epoch 5/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.4983 - acc: 0.7934 - val_loss: 0.4085 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.83916\n",
      "Epoch 6/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.9715 - acc: 0.7793 - val_loss: 0.4176 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.83916\n",
      "Epoch 7/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.8338 - acc: 0.7934 - val_loss: 0.4215 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83916 to 0.83916, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 8/200\n",
      "426/426 [==============================] - 0s 318us/step - loss: 1.7052 - acc: 0.7770 - val_loss: 0.4262 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83916\n",
      "Epoch 9/200\n",
      "426/426 [==============================] - 0s 287us/step - loss: 1.6413 - acc: 0.7840 - val_loss: 0.4311 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83916\n",
      "Epoch 10/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.3782 - acc: 0.8216 - val_loss: 0.4172 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83916\n",
      "Epoch 11/200\n",
      "426/426 [==============================] - 0s 324us/step - loss: 1.4437 - acc: 0.8005 - val_loss: 0.4299 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83916\n",
      "Epoch 12/200\n",
      "426/426 [==============================] - 0s 319us/step - loss: 1.3001 - acc: 0.8169 - val_loss: 0.4253 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83916\n",
      "Epoch 13/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.4874 - acc: 0.7958 - val_loss: 0.4288 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83916\n",
      "Epoch 14/200\n",
      "426/426 [==============================] - 0s 319us/step - loss: 1.6206 - acc: 0.7981 - val_loss: 0.4173 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83916\n",
      "Epoch 15/200\n",
      "426/426 [==============================] - 0s 309us/step - loss: 1.6914 - acc: 0.8052 - val_loss: 0.4096 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.83916 to 0.84615, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 16/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.4061 - acc: 0.7934 - val_loss: 0.4133 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84615\n",
      "Epoch 17/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.5023 - acc: 0.7934 - val_loss: 0.4101 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84615\n",
      "Epoch 18/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.6688 - acc: 0.7934 - val_loss: 0.4130 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84615\n",
      "Epoch 19/200\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.4306 - acc: 0.8192 - val_loss: 0.4113 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84615\n",
      "Epoch 20/200\n",
      "426/426 [==============================] - 0s 326us/step - loss: 1.6561 - acc: 0.7793 - val_loss: 0.3985 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.84615 to 0.84615, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 21/200\n",
      "426/426 [==============================] - 0s 331us/step - loss: 1.8139 - acc: 0.7817 - val_loss: 0.4192 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84615\n",
      "Epoch 22/200\n",
      "426/426 [==============================] - 0s 340us/step - loss: 1.6476 - acc: 0.7934 - val_loss: 0.4056 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84615\n",
      "Epoch 23/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.6705 - acc: 0.7817 - val_loss: 0.4113 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84615\n",
      "Epoch 24/200\n",
      "426/426 [==============================] - 0s 311us/step - loss: 1.6516 - acc: 0.7770 - val_loss: 0.4188 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84615\n",
      "Epoch 25/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.9190 - acc: 0.7653 - val_loss: 0.4111 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84615\n",
      "Epoch 26/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 2.1305 - acc: 0.7770 - val_loss: 0.4145 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84615\n",
      "Epoch 27/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.3971 - acc: 0.8052 - val_loss: 0.4058 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84615\n",
      "Epoch 28/200\n",
      "426/426 [==============================] - 0s 284us/step - loss: 1.8855 - acc: 0.7746 - val_loss: 0.4077 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84615\n",
      "Epoch 29/200\n",
      "426/426 [==============================] - 0s 316us/step - loss: 1.5934 - acc: 0.7864 - val_loss: 0.4109 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84615\n",
      "Epoch 30/200\n",
      "426/426 [==============================] - 0s 314us/step - loss: 1.5435 - acc: 0.7958 - val_loss: 0.4133 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84615\n",
      "Epoch 31/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.5447 - acc: 0.8052 - val_loss: 0.4103 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84615\n",
      "Epoch 32/200\n",
      "426/426 [==============================] - 0s 317us/step - loss: 1.3484 - acc: 0.8122 - val_loss: 0.4088 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84615\n",
      "Epoch 33/200\n",
      "426/426 [==============================] - 0s 321us/step - loss: 1.9246 - acc: 0.7746 - val_loss: 0.4082 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84615\n",
      "Epoch 34/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.6328 - acc: 0.7958 - val_loss: 0.4054 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84615\n",
      "Epoch 35/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 2.0954 - acc: 0.7653 - val_loss: 0.4112 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84615\n",
      "Epoch 36/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.5621 - acc: 0.7887 - val_loss: 0.4124 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84615\n",
      "Epoch 37/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.6485 - acc: 0.8028 - val_loss: 0.4142 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84615\n",
      "Epoch 38/200\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.4946 - acc: 0.8005 - val_loss: 0.4102 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84615\n",
      "Epoch 39/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.4799 - acc: 0.8005 - val_loss: 0.4201 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84615\n",
      "Epoch 40/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.7655 - acc: 0.7911 - val_loss: 0.4166 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84615\n",
      "Epoch 41/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.5746 - acc: 0.7817 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84615\n",
      "Epoch 42/200\n",
      "426/426 [==============================] - 0s 323us/step - loss: 1.7203 - acc: 0.7582 - val_loss: 0.4791 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84615\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 291us/step - loss: 1.6597 - acc: 0.7746 - val_loss: 0.4473 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84615\n",
      "Epoch 44/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.9618 - acc: 0.7629 - val_loss: 0.4375 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84615\n",
      "Epoch 45/200\n",
      "426/426 [==============================] - 0s 282us/step - loss: 1.4973 - acc: 0.7981 - val_loss: 0.4430 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84615\n",
      "Epoch 46/200\n",
      "426/426 [==============================] - 0s 291us/step - loss: 1.4957 - acc: 0.7958 - val_loss: 0.4385 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84615\n",
      "Epoch 47/200\n",
      "426/426 [==============================] - 0s 293us/step - loss: 1.7777 - acc: 0.7817 - val_loss: 0.4442 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84615\n",
      "Epoch 48/200\n",
      "426/426 [==============================] - 0s 284us/step - loss: 2.0604 - acc: 0.7629 - val_loss: 0.4525 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84615\n",
      "Epoch 49/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7310 - acc: 0.7817 - val_loss: 0.4500 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84615\n",
      "Epoch 50/200\n",
      "426/426 [==============================] - 0s 331us/step - loss: 1.6438 - acc: 0.7723 - val_loss: 0.4403 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84615\n",
      "Epoch 51/200\n",
      "426/426 [==============================] - 0s 328us/step - loss: 1.3369 - acc: 0.7770 - val_loss: 0.4301 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84615\n",
      "Epoch 52/200\n",
      "426/426 [==============================] - 0s 313us/step - loss: 1.2424 - acc: 0.8169 - val_loss: 0.4277 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84615\n",
      "Epoch 53/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.6428 - acc: 0.7840 - val_loss: 0.4084 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84615\n",
      "Epoch 54/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.4394 - acc: 0.7958 - val_loss: 0.4265 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84615\n",
      "Epoch 55/200\n",
      "426/426 [==============================] - 0s 296us/step - loss: 1.6849 - acc: 0.7793 - val_loss: 0.4188 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84615\n",
      "Epoch 56/200\n",
      "426/426 [==============================] - 0s 291us/step - loss: 1.5774 - acc: 0.7840 - val_loss: 0.4122 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84615\n",
      "Epoch 57/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.5253 - acc: 0.7746 - val_loss: 0.4207 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84615\n",
      "Epoch 58/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.4240 - acc: 0.7887 - val_loss: 0.4092 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84615\n",
      "Epoch 59/200\n",
      "426/426 [==============================] - 0s 293us/step - loss: 1.8155 - acc: 0.7817 - val_loss: 0.4047 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84615\n",
      "Epoch 60/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.5179 - acc: 0.7934 - val_loss: 0.4104 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84615\n",
      "Epoch 61/200\n",
      "426/426 [==============================] - 0s 305us/step - loss: 1.6193 - acc: 0.7746 - val_loss: 0.4179 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84615\n",
      "Epoch 62/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.7561 - acc: 0.7746 - val_loss: 0.4054 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84615\n",
      "Epoch 63/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7731 - acc: 0.7840 - val_loss: 0.4087 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84615\n",
      "Epoch 64/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.4701 - acc: 0.8122 - val_loss: 0.4219 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84615\n",
      "Epoch 65/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.7526 - acc: 0.7653 - val_loss: 0.4131 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84615\n",
      "Epoch 66/200\n",
      "426/426 [==============================] - 0s 310us/step - loss: 1.5538 - acc: 0.7864 - val_loss: 0.4085 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84615\n",
      "Epoch 67/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.5114 - acc: 0.7911 - val_loss: 0.4161 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84615\n",
      "Epoch 68/200\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.4521 - acc: 0.7887 - val_loss: 0.4236 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84615\n",
      "Epoch 69/200\n",
      "426/426 [==============================] - 0s 302us/step - loss: 1.6780 - acc: 0.7770 - val_loss: 0.4139 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84615\n",
      "Epoch 70/200\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.6911 - acc: 0.7911 - val_loss: 0.4068 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84615\n",
      "Epoch 71/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.5389 - acc: 0.7981 - val_loss: 0.4111 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84615\n",
      "Epoch 72/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.7061 - acc: 0.7676 - val_loss: 0.4139 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84615\n",
      "Epoch 73/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.5494 - acc: 0.7864 - val_loss: 0.4117 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84615\n",
      "Epoch 74/200\n",
      "426/426 [==============================] - 0s 283us/step - loss: 1.7310 - acc: 0.7887 - val_loss: 0.4169 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84615\n",
      "Epoch 75/200\n",
      "426/426 [==============================] - 0s 286us/step - loss: 1.6677 - acc: 0.7793 - val_loss: 0.4100 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84615\n",
      "Epoch 76/200\n",
      "426/426 [==============================] - 0s 281us/step - loss: 1.7747 - acc: 0.7746 - val_loss: 0.4215 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84615\n",
      "Epoch 77/200\n",
      "426/426 [==============================] - 0s 314us/step - loss: 1.6969 - acc: 0.7911 - val_loss: 0.4107 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84615\n",
      "Epoch 78/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.9980 - acc: 0.7559 - val_loss: 0.4127 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84615\n",
      "Epoch 79/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.5057 - acc: 0.7887 - val_loss: 0.4195 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84615\n",
      "Epoch 80/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.3949 - acc: 0.8028 - val_loss: 0.4365 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84615\n",
      "Epoch 81/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.8829 - acc: 0.7700 - val_loss: 0.4206 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84615\n",
      "Epoch 82/200\n",
      "426/426 [==============================] - 0s 337us/step - loss: 1.3760 - acc: 0.7887 - val_loss: 0.4322 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.84615\n",
      "Epoch 83/200\n",
      "426/426 [==============================] - 0s 302us/step - loss: 1.6641 - acc: 0.7887 - val_loss: 0.4143 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.84615\n",
      "Epoch 84/200\n",
      "426/426 [==============================] - 0s 284us/step - loss: 1.5272 - acc: 0.7676 - val_loss: 0.4063 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.84615\n",
      "Epoch 85/200\n",
      "426/426 [==============================] - 0s 289us/step - loss: 1.7068 - acc: 0.7746 - val_loss: 0.4233 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.84615\n",
      "Epoch 86/200\n",
      "426/426 [==============================] - 0s 285us/step - loss: 1.4609 - acc: 0.7934 - val_loss: 0.4224 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.84615\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 286us/step - loss: 1.7408 - acc: 0.7911 - val_loss: 0.4305 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.84615\n",
      "Epoch 88/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.7419 - acc: 0.7817 - val_loss: 0.4207 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.84615\n",
      "Epoch 89/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.3545 - acc: 0.7864 - val_loss: 0.4147 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.84615\n",
      "Epoch 90/200\n",
      "426/426 [==============================] - 0s 290us/step - loss: 1.6045 - acc: 0.7723 - val_loss: 0.4179 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.84615\n",
      "Epoch 91/200\n",
      "426/426 [==============================] - 0s 293us/step - loss: 1.8518 - acc: 0.7723 - val_loss: 0.4195 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.84615\n",
      "Epoch 92/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.5344 - acc: 0.7864 - val_loss: 0.4166 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.84615\n",
      "Epoch 93/200\n",
      "426/426 [==============================] - 0s 292us/step - loss: 2.0126 - acc: 0.7817 - val_loss: 0.4176 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.84615\n",
      "Epoch 94/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.6983 - acc: 0.7981 - val_loss: 0.4217 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.84615\n",
      "Epoch 95/200\n",
      "426/426 [==============================] - 0s 296us/step - loss: 1.6329 - acc: 0.8005 - val_loss: 0.4167 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.84615\n",
      "Epoch 96/200\n",
      "426/426 [==============================] - 0s 296us/step - loss: 1.8147 - acc: 0.7629 - val_loss: 0.4143 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.84615\n",
      "Epoch 97/200\n",
      "426/426 [==============================] - 0s 285us/step - loss: 1.6270 - acc: 0.7770 - val_loss: 0.4108 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.84615\n",
      "Epoch 98/200\n",
      "426/426 [==============================] - 0s 276us/step - loss: 1.5508 - acc: 0.7911 - val_loss: 0.4170 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.84615\n",
      "Epoch 99/200\n",
      "426/426 [==============================] - 0s 275us/step - loss: 1.5387 - acc: 0.8075 - val_loss: 0.4144 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.84615\n",
      "Epoch 100/200\n",
      "426/426 [==============================] - 0s 285us/step - loss: 1.6827 - acc: 0.7934 - val_loss: 0.4113 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.84615\n",
      "Epoch 101/200\n",
      "426/426 [==============================] - 0s 302us/step - loss: 1.5890 - acc: 0.7770 - val_loss: 0.4243 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.84615\n",
      "Epoch 102/200\n",
      "426/426 [==============================] - 0s 291us/step - loss: 1.4046 - acc: 0.8052 - val_loss: 0.4286 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.84615\n",
      "Epoch 103/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 2.0274 - acc: 0.7840 - val_loss: 0.4138 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.84615\n",
      "Epoch 104/200\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.7445 - acc: 0.7700 - val_loss: 0.4193 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.84615\n",
      "Epoch 105/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.7167 - acc: 0.7840 - val_loss: 0.4184 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.84615\n",
      "Epoch 106/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7813 - acc: 0.7911 - val_loss: 0.4142 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.84615\n",
      "Epoch 107/200\n",
      "426/426 [==============================] - 0s 284us/step - loss: 1.4042 - acc: 0.8028 - val_loss: 0.4059 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.84615\n",
      "Epoch 108/200\n",
      "426/426 [==============================] - 0s 277us/step - loss: 1.8870 - acc: 0.7723 - val_loss: 0.4052 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.84615\n",
      "Epoch 109/200\n",
      "426/426 [==============================] - 0s 286us/step - loss: 1.4597 - acc: 0.7817 - val_loss: 0.4015 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.84615\n",
      "Epoch 110/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.9294 - acc: 0.7840 - val_loss: 0.4196 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.84615\n",
      "Epoch 111/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.9658 - acc: 0.7723 - val_loss: 0.4340 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.84615\n",
      "Epoch 112/200\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.9243 - acc: 0.7606 - val_loss: 0.4272 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.84615\n",
      "Epoch 113/200\n",
      "426/426 [==============================] - 0s 302us/step - loss: 1.5839 - acc: 0.7793 - val_loss: 0.4118 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.84615\n",
      "Epoch 114/200\n",
      "426/426 [==============================] - 0s 310us/step - loss: 1.5362 - acc: 0.7817 - val_loss: 0.4192 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.84615\n",
      "Epoch 115/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.7618 - acc: 0.7864 - val_loss: 0.4155 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.84615\n",
      "Epoch 116/200\n",
      "426/426 [==============================] - 0s 295us/step - loss: 2.1927 - acc: 0.7371 - val_loss: 0.4112 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.84615\n",
      "Epoch 117/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.8540 - acc: 0.7676 - val_loss: 0.4179 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.84615\n",
      "Epoch 118/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.5488 - acc: 0.7981 - val_loss: 0.4115 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.84615\n",
      "Epoch 119/200\n",
      "426/426 [==============================] - 0s 293us/step - loss: 1.5144 - acc: 0.7723 - val_loss: 0.4092 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.84615\n",
      "Epoch 120/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 2.0704 - acc: 0.7653 - val_loss: 0.4097 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.84615\n",
      "Epoch 121/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.5214 - acc: 0.7770 - val_loss: 0.4145 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.84615\n",
      "Epoch 122/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.4325 - acc: 0.8028 - val_loss: 0.4165 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.84615\n",
      "Epoch 123/200\n",
      "426/426 [==============================] - 0s 296us/step - loss: 1.5507 - acc: 0.7958 - val_loss: 0.4176 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.84615\n",
      "Epoch 124/200\n",
      "426/426 [==============================] - 0s 290us/step - loss: 1.7364 - acc: 0.7840 - val_loss: 0.4156 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.84615\n",
      "Epoch 125/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.6195 - acc: 0.7817 - val_loss: 0.4144 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.84615\n",
      "Epoch 126/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.6209 - acc: 0.7840 - val_loss: 0.4145 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.84615\n",
      "Epoch 127/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.5407 - acc: 0.8075 - val_loss: 0.4089 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.84615\n",
      "Epoch 128/200\n",
      "426/426 [==============================] - 0s 311us/step - loss: 1.5222 - acc: 0.7864 - val_loss: 0.4166 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.84615\n",
      "Epoch 129/200\n",
      "426/426 [==============================] - 0s 306us/step - loss: 1.4589 - acc: 0.7887 - val_loss: 0.4277 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.84615\n",
      "Epoch 130/200\n",
      "426/426 [==============================] - 0s 319us/step - loss: 1.5342 - acc: 0.8005 - val_loss: 0.4191 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: val_acc did not improve from 0.84615\n",
      "Epoch 131/200\n",
      "426/426 [==============================] - 0s 327us/step - loss: 1.5268 - acc: 0.7911 - val_loss: 0.4146 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.84615\n",
      "Epoch 132/200\n",
      "426/426 [==============================] - 0s 337us/step - loss: 1.8085 - acc: 0.7770 - val_loss: 0.4175 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.84615\n",
      "Epoch 133/200\n",
      "426/426 [==============================] - 0s 320us/step - loss: 1.9548 - acc: 0.7676 - val_loss: 0.4115 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.84615\n",
      "Epoch 134/200\n",
      "426/426 [==============================] - 0s 312us/step - loss: 1.3521 - acc: 0.7934 - val_loss: 0.4159 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.84615\n",
      "Epoch 135/200\n",
      "426/426 [==============================] - 0s 305us/step - loss: 1.6855 - acc: 0.7840 - val_loss: 0.4169 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.84615\n",
      "Epoch 136/200\n",
      "426/426 [==============================] - 0s 311us/step - loss: 1.8593 - acc: 0.7817 - val_loss: 0.4184 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.84615\n",
      "Epoch 137/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.9085 - acc: 0.7653 - val_loss: 0.4181 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.84615\n",
      "Epoch 138/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.7675 - acc: 0.7793 - val_loss: 0.4281 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.84615\n",
      "Epoch 139/200\n",
      "426/426 [==============================] - 0s 312us/step - loss: 1.6851 - acc: 0.7934 - val_loss: 0.4182 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.84615\n",
      "Epoch 140/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.4400 - acc: 0.8169 - val_loss: 0.4223 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.84615\n",
      "Epoch 141/200\n",
      "426/426 [==============================] - 0s 294us/step - loss: 1.9381 - acc: 0.7746 - val_loss: 0.4159 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.84615\n",
      "Epoch 142/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.7981 - acc: 0.7700 - val_loss: 0.4168 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.84615\n",
      "Epoch 143/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 2.0604 - acc: 0.7606 - val_loss: 0.4222 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.84615\n",
      "Epoch 144/200\n",
      "426/426 [==============================] - 0s 331us/step - loss: 1.8157 - acc: 0.7770 - val_loss: 0.4220 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.84615\n",
      "Epoch 145/200\n",
      "426/426 [==============================] - 0s 312us/step - loss: 1.3794 - acc: 0.7793 - val_loss: 0.4137 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.84615\n",
      "Epoch 146/200\n",
      "426/426 [==============================] - 0s 306us/step - loss: 2.1023 - acc: 0.7723 - val_loss: 0.4151 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.84615\n",
      "Epoch 147/200\n",
      "426/426 [==============================] - 0s 316us/step - loss: 1.4846 - acc: 0.8028 - val_loss: 0.4222 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.84615\n",
      "Epoch 148/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.6123 - acc: 0.7887 - val_loss: 0.4225 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.84615\n",
      "Epoch 149/200\n",
      "426/426 [==============================] - 0s 298us/step - loss: 1.5037 - acc: 0.7934 - val_loss: 0.4117 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.84615\n",
      "Epoch 150/200\n",
      "426/426 [==============================] - 0s 322us/step - loss: 1.7132 - acc: 0.7864 - val_loss: 0.4225 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.84615\n",
      "Epoch 151/200\n",
      "426/426 [==============================] - 0s 322us/step - loss: 1.5416 - acc: 0.8075 - val_loss: 0.4160 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.84615\n",
      "Epoch 152/200\n",
      "426/426 [==============================] - 0s 310us/step - loss: 1.6119 - acc: 0.7887 - val_loss: 0.4292 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.84615\n",
      "Epoch 153/200\n",
      "426/426 [==============================] - 0s 312us/step - loss: 1.8487 - acc: 0.7746 - val_loss: 0.4311 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.84615\n",
      "Epoch 154/200\n",
      "426/426 [==============================] - 0s 312us/step - loss: 1.6143 - acc: 0.7864 - val_loss: 0.4230 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.84615\n",
      "Epoch 155/200\n",
      "426/426 [==============================] - 0s 309us/step - loss: 1.8051 - acc: 0.7700 - val_loss: 0.4170 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.84615\n",
      "Epoch 156/200\n",
      "426/426 [==============================] - 0s 320us/step - loss: 1.5981 - acc: 0.8122 - val_loss: 0.4333 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.84615\n",
      "Epoch 157/200\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.9086 - acc: 0.7746 - val_loss: 0.4304 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.84615\n",
      "Epoch 158/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.8071 - acc: 0.8005 - val_loss: 0.4277 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.84615\n",
      "Epoch 159/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.4110 - acc: 0.8005 - val_loss: 0.4184 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.84615\n",
      "Epoch 160/200\n",
      "426/426 [==============================] - 0s 301us/step - loss: 1.4755 - acc: 0.7981 - val_loss: 0.4232 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.84615\n",
      "Epoch 161/200\n",
      "426/426 [==============================] - 0s 302us/step - loss: 1.3698 - acc: 0.8075 - val_loss: 0.4236 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.84615\n",
      "Epoch 162/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7263 - acc: 0.7770 - val_loss: 0.4325 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.84615\n",
      "Epoch 163/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.7148 - acc: 0.7887 - val_loss: 0.4274 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.84615\n",
      "Epoch 164/200\n",
      "426/426 [==============================] - 0s 366us/step - loss: 1.6894 - acc: 0.7934 - val_loss: 0.4291 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.84615\n",
      "Epoch 165/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.7511 - acc: 0.7864 - val_loss: 0.4266 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.84615\n",
      "Epoch 166/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.4567 - acc: 0.7864 - val_loss: 0.4104 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.84615\n",
      "Epoch 167/200\n",
      "426/426 [==============================] - 0s 311us/step - loss: 1.3675 - acc: 0.8099 - val_loss: 0.4090 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.84615\n",
      "Epoch 168/200\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.3299 - acc: 0.7981 - val_loss: 0.4103 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.84615\n",
      "Epoch 169/200\n",
      "426/426 [==============================] - 0s 314us/step - loss: 1.9311 - acc: 0.7840 - val_loss: 0.4141 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.84615\n",
      "Epoch 170/200\n",
      "426/426 [==============================] - 0s 311us/step - loss: 1.2366 - acc: 0.8216 - val_loss: 0.4208 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.84615\n",
      "Epoch 171/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.6022 - acc: 0.7817 - val_loss: 0.4167 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.84615\n",
      "Epoch 172/200\n",
      "426/426 [==============================] - 0s 309us/step - loss: 1.5577 - acc: 0.8005 - val_loss: 0.4097 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.84615\n",
      "Epoch 173/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.7410 - acc: 0.7981 - val_loss: 0.4113 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.84615\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 312us/step - loss: 1.7169 - acc: 0.8005 - val_loss: 0.4151 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.84615\n",
      "Epoch 175/200\n",
      "426/426 [==============================] - 0s 306us/step - loss: 1.5875 - acc: 0.8099 - val_loss: 0.4192 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.84615\n",
      "Epoch 176/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7129 - acc: 0.8052 - val_loss: 0.4181 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.84615\n",
      "Epoch 177/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.8173 - acc: 0.7700 - val_loss: 0.4216 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.84615\n",
      "Epoch 178/200\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.5576 - acc: 0.7887 - val_loss: 0.4226 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.84615\n",
      "Epoch 179/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.3039 - acc: 0.8075 - val_loss: 0.4189 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.84615\n",
      "Epoch 180/200\n",
      "426/426 [==============================] - 0s 303us/step - loss: 1.4065 - acc: 0.8099 - val_loss: 0.4307 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.84615\n",
      "Epoch 181/200\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.7823 - acc: 0.7746 - val_loss: 0.4338 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.84615\n",
      "Epoch 182/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.5589 - acc: 0.8169 - val_loss: 0.4268 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.84615\n",
      "Epoch 183/200\n",
      "426/426 [==============================] - 0s 289us/step - loss: 1.6781 - acc: 0.7770 - val_loss: 0.4353 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.84615\n",
      "Epoch 184/200\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.7424 - acc: 0.8005 - val_loss: 0.4287 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.84615\n",
      "Epoch 185/200\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.4262 - acc: 0.8028 - val_loss: 0.4271 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.84615\n",
      "Epoch 186/200\n",
      "426/426 [==============================] - 0s 324us/step - loss: 1.6693 - acc: 0.7840 - val_loss: 0.4136 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.84615\n",
      "Epoch 187/200\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.6463 - acc: 0.8005 - val_loss: 0.4235 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.84615\n",
      "Epoch 188/200\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.3150 - acc: 0.8099 - val_loss: 0.4186 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.84615\n",
      "Epoch 189/200\n",
      "426/426 [==============================] - 0s 309us/step - loss: 1.5938 - acc: 0.7817 - val_loss: 0.4126 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.84615\n",
      "Epoch 190/200\n",
      "426/426 [==============================] - 0s 321us/step - loss: 1.4016 - acc: 0.8052 - val_loss: 0.4125 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.84615\n",
      "Epoch 191/200\n",
      "426/426 [==============================] - 0s 369us/step - loss: 1.5009 - acc: 0.7887 - val_loss: 0.4070 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.84615\n",
      "Epoch 192/200\n",
      "426/426 [==============================] - 0s 327us/step - loss: 1.5947 - acc: 0.8005 - val_loss: 0.4150 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.84615\n",
      "Epoch 193/200\n",
      "426/426 [==============================] - 0s 332us/step - loss: 1.3328 - acc: 0.7840 - val_loss: 0.4137 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.84615\n",
      "Epoch 194/200\n",
      "426/426 [==============================] - 0s 332us/step - loss: 1.5578 - acc: 0.7981 - val_loss: 0.4182 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.84615\n",
      "Epoch 195/200\n",
      "426/426 [==============================] - 0s 334us/step - loss: 2.0954 - acc: 0.7770 - val_loss: 0.4159 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.84615\n",
      "Epoch 196/200\n",
      "426/426 [==============================] - 0s 322us/step - loss: 1.6240 - acc: 0.8005 - val_loss: 0.4149 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.84615\n",
      "Epoch 197/200\n",
      "426/426 [==============================] - 0s 335us/step - loss: 1.4585 - acc: 0.8075 - val_loss: 0.4141 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.84615\n",
      "Epoch 198/200\n",
      "426/426 [==============================] - 0s 358us/step - loss: 1.5750 - acc: 0.7911 - val_loss: 0.4177 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.84615\n",
      "Epoch 199/200\n",
      "426/426 [==============================] - 0s 347us/step - loss: 1.3122 - acc: 0.8169 - val_loss: 0.4188 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.84615\n",
      "Epoch 200/200\n",
      "426/426 [==============================] - 0s 347us/step - loss: 2.0269 - acc: 0.7582 - val_loss: 0.4149 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.84615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4c46b990>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "filepath=\"saved_models/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "classifier.fit(X_train, y_train, validation_split=0.25, epochs=epochs, batch_size=10, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval model: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 0s 35us/step\n",
      "Accuracy using training set: 0.8418277687473331\n"
     ]
    }
   ],
   "source": [
    "# Accuracy over training set\n",
    "eval_model=classifier.evaluate(X_train, y_train)\n",
    "print(\"Accuracy using training set:\",eval_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using testing set: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# Accuracy over testing set\n",
    "classifier.load_weights('saved_models/weights.best.hdf5')\n",
    "y_pred=classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(\"Accuracy using testing set:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved an accuracy using test dataset of about **95%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have had to include several changes in the original NN architecture due to high overfitting Original architecture achieved only an about 77% accuracy using testing set, although training data accuracy was quite similar to previuos one. Some of the changes were:\n",
    "\n",
    "- Use L1 and L2 regularizations: Did not work. In fact, we got worse results.\n",
    "- Increase output size of layers. It improved accuracy (testing set) by more than 10 points\n",
    "- Change output layer activarion function from `sigmoid` to `softmax`. It worked really well.\n",
    "\n",
    "After doing that, accuracy for the preduction model using the reduced set of data is even a little bit better than using the full dataset. \n",
    "\n",
    "Probably ther are still ways for improving this values and I think that it is going to significantlly increase as we get more data. Probably, number of records in original dataset are not enought to properly train our prediction model. Anyway, one of the project conclussions is that, probably (we don't still know if meaybe new diseases are going to be included), we can make diagnosis predictions using 81 less questions (400 of the original dataset vs. 319 of the reduced one). This should be evaluated by experts in the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x1a5f1129d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5f1120d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5c139b90>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de97d10>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9f2d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de97850>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de91d10>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9f690>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9fb90>],\n",
       " <a list of 9 Text yticklabel objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have to change one hot encoded values for real prediction values\n",
    "#diagnosis_raw[np.argmax(predicted_value)]\n",
    "#confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "pred = []\n",
    "for i in y_pred :\n",
    "    pred.append(diagnosis_raw[np.argmax(i)])\n",
    "    \n",
    "test = []\n",
    "for i in y_test:\n",
    "    test.append(diagnosis_raw[np.argmax(i)])\n",
    "\n",
    "cm = confusion_matrix(test, pred)\n",
    "\n",
    "norm_conf = []\n",
    "for i in cm:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "ax.grid(False)\n",
    "res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                interpolation='nearest')\n",
    "\n",
    "width, height = cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        ax.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', color='white')\n",
    "\n",
    "cb = fig.colorbar(res)\n",
    "plt.xticks(range(width), np.unique(diagnosis_raw)[:width], rotation=90)\n",
    "plt.yticks(range(height), np.unique(diagnosis_raw)[:height])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_1 = Sequential()\n",
    "# First Hidden Layer\n",
    "classifier_1.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "classifier_1.add(Dropout(0.2))\n",
    "# Second  Hidden Layer\n",
    "classifier_1.add(Dense(32, activation='relu', kernel_initializer='random_normal'),)\n",
    "classifier_1.add(Dropout(0.2))\n",
    "# Output Layer\n",
    "classifier_1.add(Dense(number_of_diseases, activation='softmax', kernel_initializer='random_normal'))\n",
    "classifier_1.add(Dropout(0.2))\n",
    "classifier_1.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "classifier_1.load_weights('saved_models/weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDSRSLA\n"
     ]
    }
   ],
   "source": [
    "def diagnosis(answers,classifier):\n",
    "    predicted_value = classifier.predict(answers)\n",
    "    return diagnosis_raw[np.argmax(predicted_value)]\n",
    "\n",
    "print(diagnosis(X_test[:1],classifier_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 99.50%\n",
      "acc: 99.49%\n",
      "acc: 99.47%\n",
      "acc: 99.73%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 99.72%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 99.72%\n",
      "99.76% (+/- 0.21%)\n"
     ]
    }
   ],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "X = dataset\n",
    "Y = pd.read_pickle('data/diagnosis.pkl')\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    # We have to use raw data before split and then on hot encode\n",
    "    encoder.fit(Y[train])\n",
    "    encoded_Y = encoder.transform(Y[train])\n",
    "    dummy_y_train = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    encoder.fit(Y[test])\n",
    "    encoded_Y = encoder.transform(Y[test])\n",
    "    dummy_y_test = np_utils.to_categorical(encoded_Y)\n",
    "     \n",
    "    #Model\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(32, activation='relu', kernel_initializer='random_normal'),)\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(number_of_diseases, activation='softmax', kernel_initializer='random_normal'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    # Fit & evaluate\n",
    "    classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    classifier.fit(X.iloc[train], dummy_y_train, validation_split=0.25, epochs=100, batch_size=10, verbose=0)\n",
    "    scores = classifier.evaluate(X.iloc[test], dummy_y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucp",
   "language": "python",
   "name": "ucp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
