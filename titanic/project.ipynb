{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Titanic competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import pairwise_distances_argmin, silhouette_score, accuracy_score, confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers dataset has 891 samples with 12 features each.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    dataset = pd.read_csv(\"data/train.csv\")\n",
    "    print(\"Passengers dataset has {} samples with {} features each.\".format(*dataset.shape))\n",
    "except Exception as e:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing? \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a first look to dataset. We are interested in knowing number, type, range of values and empty parameters in order to take further decisions about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check how many empty values are in each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunm:  PassengerId ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Survived ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Pclass ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Name ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Sex ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Age ** Empty :  177 ** Empty percentage:  0.19865319865319866\n",
      "Colunm:  SibSp ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Parch ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Ticket ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Fare ** Empty :  0 ** Empty percentage:  0.0\n",
      "Colunm:  Cabin ** Empty :  687 ** Empty percentage:  0.7710437710437711\n",
      "Colunm:  Embarked ** Empty :  2 ** Empty percentage:  0.002244668911335578\n"
     ]
    }
   ],
   "source": [
    "for (columnName, columnData) in dataset.iteritems():\n",
    "   empty_values = columnData.isna().sum()\n",
    "   print('Colunm: ', columnName, '** Empty : ', empty_values, '** Empty percentage: ', empty_values/len(columnData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447.016393</td>\n",
       "      <td>2.531876</td>\n",
       "      <td>30.626179</td>\n",
       "      <td>0.553734</td>\n",
       "      <td>0.329690</td>\n",
       "      <td>22.117887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>444.368421</td>\n",
       "      <td>1.950292</td>\n",
       "      <td>28.343690</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>48.395408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PassengerId    Pclass        Age     SibSp     Parch       Fare\n",
       "Survived                                                                 \n",
       "0          447.016393  2.531876  30.626179  0.553734  0.329690  22.117887\n",
       "1          444.368421  1.950292  28.343690  0.473684  0.464912  48.395408"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('Survived').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin seems to be a useless feature as most (77%) of records contains an empty value. Let's check it relevance in survived column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A10</th>\n",
       "      <td>584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A14</th>\n",
       "      <td>476.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A16</th>\n",
       "      <td>557.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A19</th>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A20</th>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F33</th>\n",
       "      <td>310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F38</th>\n",
       "      <td>777.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F4</th>\n",
       "      <td>401.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G6</th>\n",
       "      <td>216.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13.581250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass    Age  SibSp  Parch       Fare\n",
       "Cabin                                                               \n",
       "A10          584.0       0.0     1.0  36.00    0.0   0.00  40.125000\n",
       "A14          476.0       0.0     1.0    NaN    0.0   0.00  52.000000\n",
       "A16          557.0       1.0     1.0  48.00    1.0   0.00  39.600000\n",
       "A19          285.0       0.0     1.0    NaN    0.0   0.00  26.000000\n",
       "A20          600.0       1.0     1.0  49.00    1.0   0.00  56.929200\n",
       "...            ...       ...     ...    ...    ...    ...        ...\n",
       "F33          310.0       1.0     2.0  29.00    0.0   0.00  11.333333\n",
       "F38          777.0       0.0     3.0    NaN    0.0   0.00   7.750000\n",
       "F4           401.5       1.0     2.0   2.50    2.0   1.00  39.000000\n",
       "G6           216.0       0.5     3.0  14.75    0.5   1.25  13.581250\n",
       "T            340.0       0.0     1.0  45.00    0.0   0.00  35.500000\n",
       "\n",
       "[147 rows x 7 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('Cabin').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely none. Let's remove it. At a first glance PassengerId seems to also be a useless feature for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('PassengerId', axis=1, inplace=True)\n",
    "dataset.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another \"suspicious\" feature is \"Name\". If it only contains a single value per record, we delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['Name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1601            7\n",
       "CA. 2343        7\n",
       "347082          7\n",
       "347088          6\n",
       "3101295         6\n",
       "               ..\n",
       "2625            1\n",
       "364498          1\n",
       "368323          1\n",
       "F.C.C. 13528    1\n",
       "28134           1\n",
       "Name: Ticket, Length: 681, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset['Ticket'].unique()))\n",
    "dataset['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch            Ticket     Fare  \\\n",
       "0         0       3    male  22.0      1      0         A/5 21171   7.2500   \n",
       "1         1       1  female  38.0      1      0          PC 17599  71.2833   \n",
       "2         1       3  female  26.0      0      0  STON/O2. 3101282   7.9250   \n",
       "3         1       1  female  35.0      1      0            113803  53.1000   \n",
       "4         0       3    male  35.0      0      0            373450   8.0500   \n",
       "5         0       3    male   NaN      0      0            330877   8.4583   \n",
       "6         0       1    male  54.0      0      0             17463  51.8625   \n",
       "7         0       3    male   2.0      3      1            349909  21.0750   \n",
       "8         1       3  female  27.0      0      2            347742  11.1333   \n",
       "9         1       2  female  14.0      1      0            237736  30.0708   \n",
       "\n",
       "  Embarked  \n",
       "0        S  \n",
       "1        C  \n",
       "2        S  \n",
       "3        S  \n",
       "4        S  \n",
       "5        Q  \n",
       "6        S  \n",
       "7        S  \n",
       "8        S  \n",
       "9        C  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110152</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110413</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>79.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110465</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110564</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110813</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W./C. 6608</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>34.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W./C. 6609</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W.E.P. 5734</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W/C 14208</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WE/P 5735</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>681 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass        Age     SibSp     Parch    Fare\n",
       "Ticket                                                              \n",
       "110152       1.000000     1.0  26.333333  0.000000  0.000000  86.500\n",
       "110413       0.666667     1.0  36.333333  0.666667  1.333333  79.650\n",
       "110465       0.000000     1.0  47.000000  0.000000  0.000000  52.000\n",
       "110564       1.000000     1.0  28.000000  0.000000  0.000000  26.550\n",
       "110813       1.000000     1.0  60.000000  1.000000  0.000000  75.250\n",
       "...               ...     ...        ...       ...       ...     ...\n",
       "W./C. 6608   0.000000     3.0  23.500000  1.500000  2.500000  34.375\n",
       "W./C. 6609   0.000000     3.0        NaN  0.000000  0.000000   7.550\n",
       "W.E.P. 5734  0.000000     1.0  46.000000  1.000000  0.000000  61.175\n",
       "W/C 14208    0.000000     2.0  30.000000  0.000000  0.000000  10.500\n",
       "WE/P 5735    0.500000     1.0  53.000000  0.500000  1.500000  71.000\n",
       "\n",
       "[681 rows x 6 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('Ticket').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S\n",
       "5         0       3    male   NaN      0      0   8.4583        Q\n",
       "6         0       1    male  54.0      0      0  51.8625        S\n",
       "7         0       3    male   2.0      3      1  21.0750        S\n",
       "8         1       3  female  27.0      0      2  11.1333        S\n",
       "9         1       2  female  14.0      1      0  30.0708        C"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop('Ticket', axis=1, inplace=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see graphically which is the disease distribution inside dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4a7d99d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['Survived'].value_counts()[:20].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Age       SibSp       Parch        Fare\n",
       "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset['Fare'].unique()))\n",
    "dataset.describe()\n",
    "#dataset.groupby('Fare').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- One hot encoding\n",
    "- To categorical\n",
    "- Data regularizarion\n",
    "Fare is some kind of cathegorical value. Let's split into 4 different categories.\n",
    "- 0 to 8\n",
    "- 8.1 to 14.5\n",
    "- 14.6 to 31\n",
    "- More than 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S\n",
       "5         0       3    male   NaN      0      0   8.4583        Q\n",
       "6         0       1    male  54.0      0      0  51.8625        S\n",
       "7         0       3    male   2.0      3      1  21.0750        S\n",
       "8         1       3  female  27.0      0      2  11.1333        S\n",
       "9         1       2  female  14.0      1      0  30.0708        C"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Sex'].replace(['female','male'],[0,1],inplace=True)\n",
    "dataset['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "885    0\n",
      "886    0\n",
      "887    1\n",
      "889    1\n",
      "890    0\n",
      "Name: Survived, Length: 712, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Survived'])\n",
    "# Split the data into features and target label\n",
    "diagnosis_raw = dataset['Survived']\n",
    "features_raw = dataset.drop('Survived', axis = 1)\n",
    "# Get number of deseases. We are going to use it for, for instance, set the output layer size in the prediction section\n",
    "number_of_diseases = len(diagnosis_raw.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let going to log transform to numerical values to ensure all values are in range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808082</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894863</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840530</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.882361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.812024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587961</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex       Age  SibSp  Parch      Fare  Embarked\n",
       "0       3    1  0.808082      1      0  0.573149       0.0\n",
       "1       1    0  0.894863      1      0  0.840530       1.0\n",
       "2       3    0  0.835552      0      0  0.585763       0.0\n",
       "3       1    0  0.882361      1      0  0.812024       0.0\n",
       "4       3    1  0.882361      0      0  0.587961       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log transform numerical value colums\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[['Age','Fare']] = features_raw[['Age','Fare']].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[['Age','Fare']] = scaler.fit_transform(features_log_transformed[['Age','Fare']])\n",
    "\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Age':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.460439</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.601092</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.538164</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.544984</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.399934</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.502583</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.390821</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.557253</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.545650</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.805569</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.805569</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.453029</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.451521</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.707770</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.414494</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.529578</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.538998</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.482071</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.427563</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.565039</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.509229</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.416739</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.439173</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416739</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.492161</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.361012</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.184964</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.593810</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.084695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.582879</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.477999</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.898244</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.556696</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.399934</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Sex       Age     SibSp     Parch      Fare  Embarked\n",
       "7       1.0  0.693147  0.184964  0.773706  0.356207  0.495832   0.63093\n",
       "10      0.0  0.000000  0.311287  0.386853  0.356207  0.460439   0.63093\n",
       "16      1.0  0.693147  0.184964  0.898244  0.356207  0.545650   0.00000\n",
       "43      0.0  0.000000  0.256106  0.386853  0.564575  0.601092   1.00000\n",
       "50      1.0  0.693147  0.427516  0.898244  0.356207  0.593810   0.63093\n",
       "58      0.0  0.000000  0.356374  0.386853  0.564575  0.538164   0.63093\n",
       "63      1.0  0.693147  0.311287  0.773706  0.564575  0.538998   0.63093\n",
       "78      1.0  0.693147  0.062728  0.000000  0.564575  0.544984   0.63093\n",
       "119     0.0  0.000000  0.184964  0.898244  0.564575  0.556696   0.63093\n",
       "164     1.0  0.693147  0.084695  0.898244  0.356207  0.593810   0.63093\n",
       "171     1.0  0.693147  0.311287  0.898244  0.356207  0.545650   0.00000\n",
       "172     0.0  0.000000  0.084695  0.386853  0.356207  0.399934   0.63093\n",
       "183     1.0  0.693147  0.084695  0.613147  0.356207  0.591080   0.63093\n",
       "184     0.0  0.000000  0.311287  0.000000  0.564575  0.502583   0.63093\n",
       "193     1.0  0.693147  0.256106  0.386853  0.356207  0.528101   0.63093\n",
       "205     0.0  0.000000  0.184964  0.000000  0.356207  0.390821   0.63093\n",
       "233     0.0  0.000000  0.356374  0.898244  0.564575  0.557253   0.63093\n",
       "261     1.0  0.693147  0.256106  0.898244  0.564575  0.557253   0.63093\n",
       "278     1.0  0.693147  0.427516  0.898244  0.356207  0.545650   0.00000\n",
       "297     0.0  0.000000  0.184964  0.386853  0.564575  0.805569   0.63093\n",
       "305     1.0  0.693147  0.074600  0.386853  0.564575  0.805569   0.63093\n",
       "340     1.0  0.693147  0.184964  0.386853  0.356207  0.528101   0.63093\n",
       "348     1.0  0.693147  0.256106  0.386853  0.356207  0.453029   0.63093\n",
       "374     0.0  0.000000  0.256106  0.773706  0.356207  0.495832   0.63093\n",
       "381     0.0  0.000000  0.084695  0.000000  0.564575  0.451521   1.00000\n",
       "386     1.0  0.693147  0.084695  1.000000  0.564575  0.619959   0.63093\n",
       "407     1.0  0.693147  0.256106  0.386853  0.356207  0.477999   0.63093\n",
       "445     1.0  0.693147  0.311287  0.000000  0.564575  0.707770   0.63093\n",
       "448     0.0  0.000000  0.356374  0.613147  0.356207  0.482071   1.00000\n",
       "469     0.0  0.000000  0.051674  0.613147  0.356207  0.482071   1.00000\n",
       "479     0.0  0.000000  0.184964  0.000000  0.356207  0.414494   0.63093\n",
       "530     0.0  0.000000  0.184964  0.386853  0.356207  0.528101   0.63093\n",
       "535     0.0  0.000000  0.427516  0.000000  0.564575  0.529578   0.63093\n",
       "618     0.0  0.000000  0.311287  0.613147  0.356207  0.591080   0.63093\n",
       "642     0.0  0.000000  0.184964  0.773706  0.564575  0.538998   0.63093\n",
       "644     0.0  0.000000  0.051674  0.613147  0.356207  0.482071   1.00000\n",
       "691     0.0  0.000000  0.311287  0.000000  0.356207  0.427563   1.00000\n",
       "720     0.0  0.000000  0.394494  0.000000  0.356207  0.565039   0.63093\n",
       "750     0.0  0.000000  0.311287  0.386853  0.356207  0.509229   0.63093\n",
       "751     1.0  0.693147  0.394494  0.000000  0.356207  0.416739   0.63093\n",
       "755     1.0  0.693147  0.040103  0.386853  0.356207  0.439173   0.63093\n",
       "777     0.0  0.000000  0.356374  0.000000  0.000000  0.416739   0.63093\n",
       "788     1.0  0.693147  0.084695  0.386853  0.564575  0.492161   0.63093\n",
       "803     1.0  0.693147  0.000000  0.000000  0.356207  0.361012   1.00000\n",
       "813     0.0  0.000000  0.394494  0.898244  0.564575  0.556696   0.63093\n",
       "824     1.0  0.693147  0.184964  0.898244  0.356207  0.593810   0.63093\n",
       "827     1.0  0.693147  0.084695  0.000000  0.564575  0.582879   1.00000\n",
       "831     1.0  0.693147  0.062728  0.386853  0.356207  0.477999   0.63093\n",
       "850     1.0  0.693147  0.311287  0.898244  0.564575  0.556696   0.63093\n",
       "869     1.0  0.693147  0.311287  0.386853  0.356207  0.399934   0.63093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points considered outliers for the feature 'Fare':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699194</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.883769</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.806239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.718988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.883769</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641423</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.893070</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709289</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.945580</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.827087</td>\n",
       "      <td>0.893450</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.880700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.806239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.799463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677677</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.893070</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.825518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Sex       Age     SibSp     Parch      Fare  Embarked\n",
       "27      1.0  0.693147  0.654108  0.773706  0.564575  0.893450   0.63093\n",
       "88      0.0  0.000000  0.699194  0.773706  0.564575  0.893450   0.63093\n",
       "118     1.0  0.693147  0.709289  0.000000  0.356207  0.883769   1.00000\n",
       "179     1.0  0.693147  0.806239  0.000000  0.000000  0.000000   0.63093\n",
       "258     0.0  0.000000  0.799463  0.000000  0.000000  1.000000   1.00000\n",
       "263     1.0  0.693147  0.831624  0.000000  0.000000  0.000000   0.63093\n",
       "271     1.0  0.693147  0.718988  0.000000  0.000000  0.000000   0.63093\n",
       "299     0.0  0.000000  0.885597  0.000000  0.356207  0.883769   1.00000\n",
       "302     1.0  0.693147  0.654108  0.000000  0.000000  0.000000   0.63093\n",
       "311     0.0  0.000000  0.641423  0.613147  0.564575  0.893070   1.00000\n",
       "341     0.0  0.000000  0.709289  0.773706  0.564575  0.893450   0.63093\n",
       "438     1.0  0.693147  0.945580  0.386853  0.827087  0.893450   0.63093\n",
       "597     1.0  0.693147  0.880700  0.000000  0.000000  0.000000   0.63093\n",
       "679     1.0  0.693147  0.806239  0.000000  0.356207  1.000000   1.00000\n",
       "737     1.0  0.693147  0.799463  0.000000  0.000000  1.000000   1.00000\n",
       "742     0.0  0.000000  0.677677  0.613147  0.564575  0.893070   1.00000\n",
       "806     1.0  0.693147  0.825518  0.000000  0.000000  0.000000   0.63093\n",
       "822     1.0  0.693147  0.819257  0.000000  0.000000  0.000000   0.63093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected in more than one feature: 0\n",
      "Original records: 712\n",
      "Records once outliers have been removed: 0\n"
     ]
    }
   ],
   "source": [
    "# For each integer feature find the data points with extreme high or low values\n",
    "# List to store outliers detected in loop\n",
    "dataset = features_log_minmax_transform\n",
    "all_outliers = []\n",
    "for feature in int_value_columns :\n",
    "    \n",
    "    Q1 = np.percentile(dataset[feature], 25)\n",
    "    Q3 = np.percentile(dataset[feature], 75)\n",
    "    step = (Q3 - Q1)*1.5\n",
    "    \n",
    "    feature_outliers = dataset[~((dataset[feature] >= Q1 - step) & (dataset[feature] <= Q3 + step))]\n",
    "    if len(feature_outliers) == 0 :\n",
    "        continue\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(feature_outliers)\n",
    "    \n",
    "    all_outliers.extend(feature_outliers.index.values)\n",
    "    \n",
    "import collections\n",
    "outliers =  [item for item, count in collections.Counter(all_outliers).items() if count > 1]\n",
    "# Remove the outliers present in more than one feature, if any were specified\n",
    "#good_data = dataset.drop(dataset.index[outliers]).reset_index(drop = True)\n",
    "print('Outliers detected in more than one feature:',len(outliers))\n",
    "print('Original records:',len(dataset))\n",
    "print('Records once outliers have been removed:',len(good_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to delete outliers in diagnosis_raw in order to keep same amount of records in features and label dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 712\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diagnosis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-6c64c709bd06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#diagnosis = diagnosis_raw.drop(dataset.index[outliers]).reset_index(drop = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original records:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagnosis_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Records once outliers have been removed:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagnosis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'diagnosis' is not defined"
     ]
    }
   ],
   "source": [
    "#diagnosis = diagnosis_raw.drop(dataset.index[outliers]).reset_index(drop = True)\n",
    "print('Original records:',len(diagnosis_raw))\n",
    "print('Records once outliers have been removed:',len(diagnosis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "At this point, we have available the following variables, that will be used extensively in the next sections:\n",
    "\n",
    "- good_data: Regularized features dataset (without outliers)\n",
    "- diagnosis: Dataset containing Labels corresponding to features dataset,\n",
    "- number_of_diseases: Number of different diseases to diagnose\n",
    "\n",
    "We are going to store these datasets in files on order to use them throughout next sections without needing to execute all previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.to_pickle('data/good_data.pkl')\n",
    "diagnosis_raw.to_pickle('data/diagnosis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the project is to provide doctors a prediction function that helps them to diagnose a TMD disorder based on patient answers to questionaries. Along previous steps, we have declared a set of variables that are going to be useful for the prediction model.\n",
    "\n",
    "- **dataset**: Imported from `data/good_data.pkl` file. Regularized features dataset (without outliers) \n",
    "- **diagnosis**: Imported from `data/diagnosis_raw.pkl` file. Dataset containing Labels corresponding to features dataset.\n",
    "- **number_of_diseases**: Number of different diseases to diagnose\n",
    "- **reduced_dataset**: Imported from `data/good_data_reduced.pkl` file. Regularized features dataset without columns deleted in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n",
      "712\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_pickle('data/good_data.pkl')\n",
    "diagnosis = pd.read_pickle('data/diagnosis.pkl')\n",
    "print(len(dataset))\n",
    "print(len(diagnosis))\n",
    "print(len(dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     0\n",
       "6     0\n",
       "7     0\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label dataset one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply one hot encoding to diagnosis labels (as we are dealing with 11 different labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "#encoder = LabelEncoder()\n",
    "#encoder.fit(diagnosis)\n",
    "#encoded_Y = encoder.transform(diagnosis)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex       Age  SibSp  Parch      Fare  Embarked\n",
      "320       3    1  0.808082      0      0  0.573149       0.0\n",
      "608       2    0  0.808082      1      2  0.787187       1.0\n",
      "443       2    0  0.847466      0      0  0.652469       0.0\n",
      "261       3    1  0.411005      4      2  0.757231       0.0\n",
      "392       3    1  0.847466      2      0  0.585763       0.0\n",
      "..      ...  ...       ...    ...    ...       ...       ...\n",
      "289       3    0  0.808082      0      0  0.582616       2.0\n",
      "92        1    1  0.923193      1      0  0.825911       0.0\n",
      "401       3    1  0.835552      0      0  0.587961       0.0\n",
      "834       3    1  0.773925      0      0  0.592240       0.0\n",
      "621       1    1  0.909826      1      0  0.810997       0.0\n",
      "\n",
      "[569 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, diagnosis, test_size=0.2)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "# First Hidden Layer\n",
    "classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "classifier.add(Dropout(0.2))\n",
    "# Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "classifier.compile(optimizer ='nadam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/ucp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "426/426 [==============================] - 2s 4ms/step - loss: 2.1688 - acc: 0.5446 - val_loss: 0.6379 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61538, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 284us/step - loss: 1.7073 - acc: 0.6479 - val_loss: 0.6026 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.61538\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 288us/step - loss: 1.9496 - acc: 0.6948 - val_loss: 0.5831 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61538\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.5859 - acc: 0.7347 - val_loss: 0.5380 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.61538 to 0.74126, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 286us/step - loss: 1.5873 - acc: 0.7394 - val_loss: 0.5005 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74126 to 0.78322, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.4231 - acc: 0.7488 - val_loss: 0.4859 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78322\n",
      "Epoch 7/100\n",
      "426/426 [==============================] - 0s 285us/step - loss: 2.0393 - acc: 0.7324 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78322\n",
      "Epoch 8/100\n",
      "426/426 [==============================] - 0s 424us/step - loss: 1.4847 - acc: 0.7512 - val_loss: 0.4776 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.78322 to 0.79021, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 9/100\n",
      "426/426 [==============================] - 0s 345us/step - loss: 1.8482 - acc: 0.7254 - val_loss: 0.4693 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79021\n",
      "Epoch 10/100\n",
      "426/426 [==============================] - 0s 321us/step - loss: 1.7914 - acc: 0.7418 - val_loss: 0.4846 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79021\n",
      "Epoch 11/100\n",
      "426/426 [==============================] - 0s 309us/step - loss: 1.6013 - acc: 0.7512 - val_loss: 0.4724 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79021\n",
      "Epoch 12/100\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.7683 - acc: 0.7347 - val_loss: 0.4738 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79021\n",
      "Epoch 13/100\n",
      "426/426 [==============================] - 0s 311us/step - loss: 2.1012 - acc: 0.7136 - val_loss: 0.4639 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79021\n",
      "Epoch 14/100\n",
      "426/426 [==============================] - 0s 287us/step - loss: 1.9991 - acc: 0.7230 - val_loss: 0.4765 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79021\n",
      "Epoch 15/100\n",
      "426/426 [==============================] - 0s 280us/step - loss: 1.8344 - acc: 0.7394 - val_loss: 0.4598 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.79021 to 0.80420, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 16/100\n",
      "426/426 [==============================] - 0s 372us/step - loss: 1.5359 - acc: 0.7394 - val_loss: 0.4601 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.80420 to 0.81119, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 17/100\n",
      "426/426 [==============================] - 0s 300us/step - loss: 1.9221 - acc: 0.7277 - val_loss: 0.4773 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81119\n",
      "Epoch 18/100\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.9202 - acc: 0.7136 - val_loss: 0.4682 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81119\n",
      "Epoch 19/100\n",
      "426/426 [==============================] - 0s 331us/step - loss: 2.0194 - acc: 0.7230 - val_loss: 0.5010 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.81119\n",
      "Epoch 20/100\n",
      "426/426 [==============================] - 0s 307us/step - loss: 1.5949 - acc: 0.7371 - val_loss: 0.4800 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.81119\n",
      "Epoch 21/100\n",
      "426/426 [==============================] - 0s 308us/step - loss: 1.5218 - acc: 0.7418 - val_loss: 0.4703 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.81119\n",
      "Epoch 22/100\n",
      "426/426 [==============================] - 0s 270us/step - loss: 2.0175 - acc: 0.7394 - val_loss: 0.4718 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.81119\n",
      "Epoch 23/100\n",
      "426/426 [==============================] - 0s 254us/step - loss: 1.6063 - acc: 0.7347 - val_loss: 0.4622 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81119\n",
      "Epoch 24/100\n",
      "426/426 [==============================] - 0s 243us/step - loss: 2.0156 - acc: 0.7394 - val_loss: 0.4663 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81119\n",
      "Epoch 25/100\n",
      "426/426 [==============================] - 0s 304us/step - loss: 1.5675 - acc: 0.7347 - val_loss: 0.4700 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81119\n",
      "Epoch 26/100\n",
      "426/426 [==============================] - 0s 285us/step - loss: 1.8535 - acc: 0.7207 - val_loss: 0.4658 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81119\n",
      "Epoch 27/100\n",
      "426/426 [==============================] - 0s 278us/step - loss: 1.7553 - acc: 0.7230 - val_loss: 0.5431 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81119\n",
      "Epoch 28/100\n",
      "426/426 [==============================] - 0s 251us/step - loss: 1.7021 - acc: 0.7465 - val_loss: 0.4950 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81119\n",
      "Epoch 29/100\n",
      "426/426 [==============================] - 0s 270us/step - loss: 1.6025 - acc: 0.7324 - val_loss: 0.4850 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81119\n",
      "Epoch 30/100\n",
      "426/426 [==============================] - 0s 255us/step - loss: 1.8172 - acc: 0.7230 - val_loss: 0.4804 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81119\n",
      "Epoch 31/100\n",
      "426/426 [==============================] - 0s 262us/step - loss: 1.4918 - acc: 0.7535 - val_loss: 0.4694 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81119\n",
      "Epoch 32/100\n",
      "426/426 [==============================] - 0s 279us/step - loss: 1.7241 - acc: 0.7441 - val_loss: 0.4685 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81119\n",
      "Epoch 33/100\n",
      "426/426 [==============================] - 0s 272us/step - loss: 1.4384 - acc: 0.7394 - val_loss: 0.4654 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81119\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 276us/step - loss: 1.5426 - acc: 0.7230 - val_loss: 0.4693 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81119\n",
      "Epoch 35/100\n",
      "426/426 [==============================] - 0s 238us/step - loss: 1.6649 - acc: 0.7441 - val_loss: 0.4766 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81119\n",
      "Epoch 36/100\n",
      "426/426 [==============================] - 0s 240us/step - loss: 1.7937 - acc: 0.7441 - val_loss: 0.4751 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81119\n",
      "Epoch 37/100\n",
      "426/426 [==============================] - 0s 237us/step - loss: 1.7367 - acc: 0.7300 - val_loss: 0.4640 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81119\n",
      "Epoch 38/100\n",
      "426/426 [==============================] - 0s 251us/step - loss: 1.6782 - acc: 0.7465 - val_loss: 0.4671 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81119\n",
      "Epoch 39/100\n",
      "426/426 [==============================] - 0s 324us/step - loss: 1.5036 - acc: 0.7394 - val_loss: 0.4637 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81119\n",
      "Epoch 40/100\n",
      "426/426 [==============================] - 0s 263us/step - loss: 1.8000 - acc: 0.7606 - val_loss: 0.4571 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81119\n",
      "Epoch 41/100\n",
      "426/426 [==============================] - 0s 244us/step - loss: 1.6793 - acc: 0.7512 - val_loss: 0.4521 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81119\n",
      "Epoch 42/100\n",
      "426/426 [==============================] - 0s 242us/step - loss: 1.9237 - acc: 0.7066 - val_loss: 0.4658 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81119\n",
      "Epoch 43/100\n",
      "426/426 [==============================] - 0s 250us/step - loss: 1.7446 - acc: 0.7582 - val_loss: 0.4566 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81119\n",
      "Epoch 44/100\n",
      "426/426 [==============================] - 0s 249us/step - loss: 1.7978 - acc: 0.7254 - val_loss: 0.4576 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.81119\n",
      "Epoch 45/100\n",
      "426/426 [==============================] - 0s 246us/step - loss: 1.5744 - acc: 0.7512 - val_loss: 0.4628 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.81119\n",
      "Epoch 46/100\n",
      "426/426 [==============================] - 0s 247us/step - loss: 1.8404 - acc: 0.7394 - val_loss: 0.4659 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.81119\n",
      "Epoch 47/100\n",
      "426/426 [==============================] - 0s 254us/step - loss: 1.7239 - acc: 0.7300 - val_loss: 0.4651 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.81119\n",
      "Epoch 48/100\n",
      "426/426 [==============================] - 0s 243us/step - loss: 1.3696 - acc: 0.7488 - val_loss: 0.4598 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.81119\n",
      "Epoch 49/100\n",
      "426/426 [==============================] - 0s 241us/step - loss: 1.8145 - acc: 0.7512 - val_loss: 0.4580 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.81119\n",
      "Epoch 50/100\n",
      "426/426 [==============================] - 0s 245us/step - loss: 2.0167 - acc: 0.7207 - val_loss: 0.4698 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.81119\n",
      "Epoch 51/100\n",
      "426/426 [==============================] - 0s 255us/step - loss: 1.6377 - acc: 0.7347 - val_loss: 0.4609 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.81119\n",
      "Epoch 52/100\n",
      "426/426 [==============================] - 0s 248us/step - loss: 1.8865 - acc: 0.7559 - val_loss: 0.4592 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.81119\n",
      "Epoch 53/100\n",
      "426/426 [==============================] - 0s 244us/step - loss: 1.9345 - acc: 0.7371 - val_loss: 0.4542 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.81119\n",
      "Epoch 54/100\n",
      "426/426 [==============================] - 0s 249us/step - loss: 1.6749 - acc: 0.7465 - val_loss: 0.4589 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.81119\n",
      "Epoch 55/100\n",
      "426/426 [==============================] - 0s 264us/step - loss: 2.2303 - acc: 0.7160 - val_loss: 0.4538 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.81119\n",
      "Epoch 56/100\n",
      "426/426 [==============================] - 0s 250us/step - loss: 1.6147 - acc: 0.7676 - val_loss: 0.4507 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.81119\n",
      "Epoch 57/100\n",
      "426/426 [==============================] - 0s 287us/step - loss: 1.5224 - acc: 0.7559 - val_loss: 0.4658 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.81119\n",
      "Epoch 58/100\n",
      "426/426 [==============================] - 0s 258us/step - loss: 1.7582 - acc: 0.7488 - val_loss: 0.4524 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.81119\n",
      "Epoch 59/100\n",
      "426/426 [==============================] - 0s 250us/step - loss: 1.8557 - acc: 0.7371 - val_loss: 0.4594 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.81119\n",
      "Epoch 60/100\n",
      "426/426 [==============================] - 0s 260us/step - loss: 1.8915 - acc: 0.7300 - val_loss: 0.4585 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.81119\n",
      "Epoch 61/100\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.3494 - acc: 0.7488 - val_loss: 0.4553 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.81119\n",
      "Epoch 62/100\n",
      "426/426 [==============================] - 0s 298us/step - loss: 2.0303 - acc: 0.7183 - val_loss: 0.4612 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.81119\n",
      "Epoch 63/100\n",
      "426/426 [==============================] - 0s 299us/step - loss: 1.9010 - acc: 0.7582 - val_loss: 0.4575 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.81119\n",
      "Epoch 64/100\n",
      "426/426 [==============================] - 0s 281us/step - loss: 1.5737 - acc: 0.7418 - val_loss: 0.4535 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.81119\n",
      "Epoch 65/100\n",
      "426/426 [==============================] - 0s 265us/step - loss: 1.4992 - acc: 0.7488 - val_loss: 0.4629 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.81119\n",
      "Epoch 66/100\n",
      "426/426 [==============================] - 0s 246us/step - loss: 1.8335 - acc: 0.7465 - val_loss: 0.4560 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.81119\n",
      "Epoch 67/100\n",
      "426/426 [==============================] - 0s 237us/step - loss: 1.5722 - acc: 0.7512 - val_loss: 0.4510 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.81119\n",
      "Epoch 68/100\n",
      "426/426 [==============================] - 0s 247us/step - loss: 1.7466 - acc: 0.7535 - val_loss: 0.4526 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.81119\n",
      "Epoch 69/100\n",
      "426/426 [==============================] - 0s 245us/step - loss: 1.5545 - acc: 0.7606 - val_loss: 0.4573 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.81119\n",
      "Epoch 70/100\n",
      "426/426 [==============================] - 0s 249us/step - loss: 1.6739 - acc: 0.7559 - val_loss: 0.4542 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.81119\n",
      "Epoch 71/100\n",
      "426/426 [==============================] - 0s 247us/step - loss: 1.7623 - acc: 0.7324 - val_loss: 0.4579 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.81119\n",
      "Epoch 72/100\n",
      "426/426 [==============================] - 0s 246us/step - loss: 1.9139 - acc: 0.7441 - val_loss: 0.4637 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.81119\n",
      "Epoch 73/100\n",
      "426/426 [==============================] - 0s 254us/step - loss: 1.9882 - acc: 0.7465 - val_loss: 0.4608 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.81119\n",
      "Epoch 74/100\n",
      "426/426 [==============================] - 0s 246us/step - loss: 1.3470 - acc: 0.7770 - val_loss: 0.4584 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.81119\n",
      "Epoch 75/100\n",
      "426/426 [==============================] - 0s 244us/step - loss: 1.8284 - acc: 0.7371 - val_loss: 0.4546 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.81119\n",
      "Epoch 76/100\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.7914 - acc: 0.7512 - val_loss: 0.4554 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.81119\n",
      "Epoch 77/100\n",
      "426/426 [==============================] - 0s 255us/step - loss: 1.5543 - acc: 0.7512 - val_loss: 0.4660 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.81119\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 259us/step - loss: 1.3118 - acc: 0.7840 - val_loss: 0.4536 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.81119\n",
      "Epoch 79/100\n",
      "426/426 [==============================] - 0s 252us/step - loss: 2.2167 - acc: 0.7324 - val_loss: 0.4662 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.81119\n",
      "Epoch 80/100\n",
      "426/426 [==============================] - 0s 269us/step - loss: 1.7842 - acc: 0.7535 - val_loss: 0.4514 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.81119\n",
      "Epoch 81/100\n",
      "426/426 [==============================] - 0s 276us/step - loss: 1.6820 - acc: 0.7465 - val_loss: 0.4659 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.81119\n",
      "Epoch 82/100\n",
      "426/426 [==============================] - 0s 290us/step - loss: 1.8834 - acc: 0.7394 - val_loss: 0.4580 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.81119\n",
      "Epoch 83/100\n",
      "426/426 [==============================] - 0s 258us/step - loss: 1.4925 - acc: 0.7629 - val_loss: 0.4549 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.81119\n",
      "Epoch 84/100\n",
      "426/426 [==============================] - 0s 268us/step - loss: 1.6709 - acc: 0.7653 - val_loss: 0.4593 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.81119\n",
      "Epoch 85/100\n",
      "426/426 [==============================] - 0s 270us/step - loss: 1.5907 - acc: 0.7606 - val_loss: 0.4545 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.81119\n",
      "Epoch 86/100\n",
      "426/426 [==============================] - 0s 262us/step - loss: 1.7166 - acc: 0.7465 - val_loss: 0.4557 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.81119\n",
      "Epoch 87/100\n",
      "426/426 [==============================] - 0s 262us/step - loss: 1.6489 - acc: 0.7441 - val_loss: 0.4503 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.81119\n",
      "Epoch 88/100\n",
      "426/426 [==============================] - 0s 289us/step - loss: 1.5211 - acc: 0.7676 - val_loss: 0.4507 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.81119\n",
      "Epoch 89/100\n",
      "426/426 [==============================] - 0s 315us/step - loss: 1.4789 - acc: 0.7700 - val_loss: 0.4592 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.81119\n",
      "Epoch 90/100\n",
      "426/426 [==============================] - 0s 279us/step - loss: 1.7969 - acc: 0.7488 - val_loss: 0.4516 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.81119\n",
      "Epoch 91/100\n",
      "426/426 [==============================] - 0s 284us/step - loss: 1.7544 - acc: 0.7512 - val_loss: 0.4570 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.81119\n",
      "Epoch 92/100\n",
      "426/426 [==============================] - 0s 274us/step - loss: 1.3901 - acc: 0.7817 - val_loss: 0.4452 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.81119\n",
      "Epoch 93/100\n",
      "426/426 [==============================] - 0s 264us/step - loss: 1.8740 - acc: 0.7488 - val_loss: 0.4510 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.81119\n",
      "Epoch 94/100\n",
      "426/426 [==============================] - 0s 266us/step - loss: 1.3718 - acc: 0.7582 - val_loss: 0.4438 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.81119\n",
      "Epoch 95/100\n",
      "426/426 [==============================] - 0s 285us/step - loss: 1.7027 - acc: 0.7582 - val_loss: 0.4854 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.81119\n",
      "Epoch 96/100\n",
      "426/426 [==============================] - 0s 289us/step - loss: 1.8018 - acc: 0.7606 - val_loss: 0.4565 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.81119\n",
      "Epoch 97/100\n",
      "426/426 [==============================] - 0s 295us/step - loss: 1.5629 - acc: 0.7746 - val_loss: 0.4452 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.81119\n",
      "Epoch 98/100\n",
      "426/426 [==============================] - 0s 305us/step - loss: 1.6283 - acc: 0.7465 - val_loss: 0.4394 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.81119\n",
      "Epoch 99/100\n",
      "426/426 [==============================] - 0s 292us/step - loss: 1.9131 - acc: 0.7418 - val_loss: 0.4374 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00099: val_acc improved from 0.81119 to 0.81818, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 100/100\n",
      "426/426 [==============================] - 0s 297us/step - loss: 1.5957 - acc: 0.7606 - val_loss: 0.4448 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.81818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4d1c8d90>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "filepath=\"saved_models/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "classifier.fit(X_train, y_train, validation_split=0.25, epochs=epochs, batch_size=10, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval model: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 0s 25us/step\n",
      "Accuracy using training set: 0.8084358519535702\n"
     ]
    }
   ],
   "source": [
    "# Accuracy over training set\n",
    "eval_model=classifier.evaluate(X_train, y_train)\n",
    "print(\"Accuracy using training set:\",eval_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using testing set: 0.8041958041958042\n"
     ]
    }
   ],
   "source": [
    "# Accuracy over testing set\n",
    "classifier.load_weights('saved_models/weights.best.hdf5')\n",
    "y_pred=classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(\"Accuracy using testing set:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved an accuracy using test dataset of about **95%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have had to include several changes in the original NN architecture due to high overfitting Original architecture achieved only an about 77% accuracy using testing set, although training data accuracy was quite similar to previuos one. Some of the changes were:\n",
    "\n",
    "- Use L1 and L2 regularizations: Did not work. In fact, we got worse results.\n",
    "- Increase output size of layers. It improved accuracy (testing set) by more than 10 points\n",
    "- Change output layer activarion function from `sigmoid` to `softmax`. It worked really well.\n",
    "\n",
    "After doing that, accuracy for the preduction model using the reduced set of data is even a little bit better than using the full dataset. \n",
    "\n",
    "Probably ther are still ways for improving this values and I think that it is going to significantlly increase as we get more data. Probably, number of records in original dataset are not enought to properly train our prediction model. Anyway, one of the project conclussions is that, probably (we don't still know if meaybe new diseases are going to be included), we can make diagnosis predictions using 81 less questions (400 of the original dataset vs. 319 of the reduced one). This should be evaluated by experts in the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x1a5f1129d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5f1120d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5c139b90>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de97d10>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9f2d0>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de97850>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de91d10>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9f690>,\n",
       "  <matplotlib.axis.YTick at 0x1a5de9fb90>],\n",
       " <a list of 9 Text yticklabel objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have to change one hot encoded values for real prediction values\n",
    "#diagnosis_raw[np.argmax(predicted_value)]\n",
    "#confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "pred = []\n",
    "for i in y_pred :\n",
    "    pred.append(diagnosis_raw[np.argmax(i)])\n",
    "    \n",
    "test = []\n",
    "for i in y_test:\n",
    "    test.append(diagnosis_raw[np.argmax(i)])\n",
    "\n",
    "cm = confusion_matrix(test, pred)\n",
    "\n",
    "norm_conf = []\n",
    "for i in cm:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "ax.grid(False)\n",
    "res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                interpolation='nearest')\n",
    "\n",
    "width, height = cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        ax.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', color='white')\n",
    "\n",
    "cb = fig.colorbar(res)\n",
    "plt.xticks(range(width), np.unique(diagnosis_raw)[:width], rotation=90)\n",
    "plt.yticks(range(height), np.unique(diagnosis_raw)[:height])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_1 = Sequential()\n",
    "# First Hidden Layer\n",
    "classifier_1.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "classifier_1.add(Dropout(0.2))\n",
    "# Second  Hidden Layer\n",
    "classifier_1.add(Dense(32, activation='relu', kernel_initializer='random_normal'),)\n",
    "classifier_1.add(Dropout(0.2))\n",
    "# Output Layer\n",
    "classifier_1.add(Dense(number_of_diseases, activation='softmax', kernel_initializer='random_normal'))\n",
    "classifier_1.add(Dropout(0.2))\n",
    "classifier_1.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "classifier_1.load_weights('saved_models/weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDSRSLA\n"
     ]
    }
   ],
   "source": [
    "def diagnosis(answers,classifier):\n",
    "    predicted_value = classifier.predict(answers)\n",
    "    return diagnosis_raw[np.argmax(predicted_value)]\n",
    "\n",
    "print(diagnosis(X_test[:1],classifier_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 99.50%\n",
      "acc: 99.49%\n",
      "acc: 99.47%\n",
      "acc: 99.73%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 99.72%\n",
      "acc: 100.00%\n",
      "acc: 100.00%\n",
      "acc: 99.72%\n",
      "99.76% (+/- 0.21%)\n"
     ]
    }
   ],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "X = dataset\n",
    "Y = pd.read_pickle('data/diagnosis.pkl')\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    # We have to use raw data before split and then on hot encode\n",
    "    encoder.fit(Y[train])\n",
    "    encoded_Y = encoder.transform(Y[train])\n",
    "    dummy_y_train = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    encoder.fit(Y[test])\n",
    "    encoded_Y = encoder.transform(Y[test])\n",
    "    dummy_y_test = np_utils.to_categorical(encoded_Y)\n",
    "     \n",
    "    #Model\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=len(dataset.columns)))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(32, activation='relu', kernel_initializer='random_normal'),)\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Dense(number_of_diseases, activation='softmax', kernel_initializer='random_normal'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    # Fit & evaluate\n",
    "    classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    classifier.fit(X.iloc[train], dummy_y_train, validation_split=0.25, epochs=100, batch_size=10, verbose=0)\n",
    "    scores = classifier.evaluate(X.iloc[test], dummy_y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucp",
   "language": "python",
   "name": "ucp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
